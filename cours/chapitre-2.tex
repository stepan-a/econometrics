\synctex=1

\documentclass[10pt]{beamer}

\usepackage[T1]{fontenc}
\usepackage{etex}
\usepackage{fourier-orns}
\usepackage{ccicons}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsbsy}
\usepackage{amsopn}
\usepackage{amscd}
\usepackage{amsxtra}
\usepackage{amsthm}
\usepackage{float}
\usepackage{color, colortbl}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage[nice]{nicefrac}
\usepackage{setspace}
\usepackage{ragged2e}
\usepackage{listings}
\usepackage{algorithms/algorithm}
\usepackage{algorithms/algorithmic}
\usepackage{tikz,pgfplots,pgfplotstable}
\pgfplotsset{compat=1.18}%newest}
\usetikzlibrary{patterns, arrows, decorations.pathreplacing, decorations.markings, calc}
\pgfplotsset{plot coordinates/math parser=false}
\usetikzlibrary{external}
\tikzexternalize[prefix=figures/]
\newlength\figureheight
\newlength\figurewidth
\usepackage{cancel}
\usepackage{tikz-qtree}
\usepackage{dcolumn}
\usepackage{adjustbox}
\usepackage{environ}
\usepackage[cal=boondox]{mathalfa}
\usepackage{manfnt}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  filecolor=black,
  urlcolor=blue,
}
\usepackage{venndiagram}
\usepackage{subcaption}
\usepackage{centernot}

\usepackage[backend=biber,style=bwl-FU,natbib=true,doi=false,isbn=false,url=false,eprint=false]{biblatex}%bwl-FU
\addbibresource{econometrics.bib}

\makeatletter
\@ifclassloaded{beamer}{
\usefonttheme[onlymath]{serif}
\uselanguage{French}
\languagepath{French}
% Git hash
\usepackage{xstring}
\usepackage{catchfile}
\immediate\write18{git rev-parse HEAD > git.hash}
\CatchFileDef{\HEAD}{git.hash}{\endlinechar=-1}
\newcommand{\gitrevision}{\StrLeft{\HEAD}{7}}
}{}
\makeatother

\newcommand{\trace}{\mathrm{tr}}
\newcommand{\vect}{\mathrm{vec}}
\newcommand{\tracarg}[1]{\mathrm{tr}\left\{#1\right\}}
\newcommand{\vectarg}[1]{\mathrm{vec}\left(#1\right)}
\newcommand{\vecth}[1]{\mathrm{vech}\left(#1\right)}
\newcommand{\iid}[2]{\mathrm{iid}\left(#1,#2\right)}
\newcommand{\normal}[2]{\mathcal N\left(#1,#2\right)}
\newcommand{\sample}{\mathcal Y_T}
\newcommand{\samplet}[1]{\mathcal Y_{#1}}
\newcommand{\slidetitle}[1]{\fancyhead[L]{\textsc{#1}}}

\newcommand{\R}{{\mathbb R}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\binomial}[2]{\begin{pmatrix} #1 \\ #2 \end{pmatrix}}
\newcommand{\bigO}[1]{\mathcal O \left(#1\right)}
\newcommand{\red}{\color{red}}
\newcommand{\blue}{\color{blue}}
\newcommand{\plim}{\overset{\text{proba}}{\underset{T\rightarrow\infty}{\longrightarrow}}}
\newcommand{\epsvar}{\sigma_{\varepsilon}^2}


\newcommand\gauss[2]{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))} % Gaussian probability density function.

\renewcommand{\qedsymbol}{C.Q.F.D.}

\newcolumntype{d}{D{.}{.}{-1}}
\definecolor{gray}{gray}{0.9}
\newcolumntype{g}{>{\columncolor{gray}}c}


\makeatletter
\@ifclassloaded{beamer}{\setbeamertemplate{theorems}[numbered]{}}{}
\makeatother

\theoremstyle{plain}

\makeatletter
\@ifclassloaded{beamer}{
\setbeamertemplate{footline}{
  {\hfill\vspace*{1pt}\href{http://creativecommons.org/licenses/by-sa/3.0/legalcode}{\ccbysa}\hspace{.1cm}
    \href{https://github.com/stepan-a/econometrics/blob/\HEAD/cours/chapitre-2.tex}{\gitrevision}\enspace--\enspace\today\enspace
  }}

\makeatother


\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{blocks}[rounded][shadow=true]
\setbeamertemplate{caption}[numbered]

\NewEnviron{notes}{\justifying\footnotesize\begin{spacing}{1.0}\BODY\vfill\pagebreak\end{spacing}}

\newenvironment{exercise}[1]
{\bgroup \small\begin{block}{Ex. #1}}
  {\end{block}\egroup}

\newenvironment{defn}[1]
{\bgroup \small\begin{block}{Définition. #1}}
  {\end{block}\egroup}

\newenvironment{exemple}[1]
{\bgroup \small\begin{block}{Exemple. #1}}
  {\end{block}\egroup}
}{}

\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollaire}

%\usepgfplotslibrary{external}
%\tikzexternalize


\begin{document}

\title{Économétrie\\\small{Violations des conditions idéales}}
\author[S. Adjemian]{Stéphane Adjemian}
\institute{\texttt{stephane.adjemian@univ-lemans.fr}}
\date{Septembre 2024}

\begin{frame}
  \titlepage{}
\end{frame}


\begin{frame}
  \frametitle{Les conditions idéales}

  Dans le chapitre précédent, nous avons supposé que~:\newline

  \medskip

  \begin{itemize}

  \item Les erreurs sont centrées, \textit{ie} $\mathbb E[\varepsilon]=0$.\newline

  \item Le modèle empirique est bien spécifié.\newline

  \item Les erreurs sont normalement distruées.\newline

  \item La matrice $X$ est de plein rang colonne.\newline

  \item Les erreurs sont homoscédastiques et non autocorrélées, \textit{ie}  $\mathbb V[\varepsilon] = \epsvar I_T$.\newline

  \end{itemize}

  \medskip

  $\Rightarrow$ Comment se comporte l'estimateur des MCO si ces conditions ne sont pas réunies~?

\end{frame}


\begin{frame}
  \frametitle{Erreurs non centrées, I}

  \begin{prop}\label{prop:bhat:non-zero-mean-error}
    Supposons que $\mathbb E\left[\varepsilon\right] = \xi \neq 0$, alors~:
    \begin{enumerate}

    \item $\mathbb E\left[ \hat{\mathbf b} \right] = \beta + (X'X)^{-1}X'\xi$
    \item $\hat{\mathbf b} \plim \beta + Q^{-1}\underset{T\rightarrow\infty}{\lim} \frac{X'\xi}{T}$

    \end{enumerate}
  \end{prop}

  \bigskip

  \begin{itemize}

  \item Les colonnes de $X$ sont linéairement indépendantes $\Rightarrow$$X'\xi=0$ ssi $\xi=0$.\newline

  \item $\hat{\mathbf b}$ est un estimateur sans biais de $\beta$ ssi $\xi=0$.\newline

  \item \textbf{Remarque:} A priori $\xi$ est différent de $\mathbf 1 \triangleq \begin{pmatrix}1 & 1 & \dots & 1\end{pmatrix}'$.

  \end{itemize}

\end{frame}


\begin{notes}
  \textbf{Preuve de la proposition \ref{prop:bhat:non-zero-mean-error}.} Supposons que $\varepsilon = \nu + \xi$ où $\nu$ est un vecteur gaussien d'espérance nulle et de variance $\sigma_{\varepsilon}^2I_T$. En substituant le DGP dans l'espression de l'estimateur des MCO, il vient~:
  \[
    \begin{split}
      \hat{\mathbf b}_T &= (X'X)^{-1}X'\left( X\beta + \xi + \nu\right)\\
      &= \beta + (X'X)^{-1}X'\xi + (X'X)^{-1}X'\nu
    \end{split}
  \]
  En prenant l'espérance, on obtient directement~:
  \[
    \mathbb E\left[ \hat{\mathbf b} \right] = \beta + (X'X)^{-1}X'\xi
  \]
  puisque le vecteur $\nu$ est d'espérance nulle. On a aussi~:
  \[
    \hat{\mathbf b} \plim \beta + Q^{-1}\underset{T\rightarrow\infty}{\lim}\frac{X'\xi}{T} + Q^{-1}\underset{T\rightarrow\infty}{\text{plim}}\frac{X'\nu}{T}
  \]
  On peut montrer que le dernier terme est nul. En effet $\mathbb E\left[\frac{X'\nu}{T}\right]=0$ et
  \[
    \mathbb V \left[ \frac{X'\nu}{T} \right] = \frac{\epsvar}{T}\frac{X'X}{T} \underset{T\rightarrow\infty}{\longrightarrow} 0\times Q = 0
  \]
  Ainsi nous avons bien~:
  \[
    \hat{\mathbf b} \plim \beta + Q^{-1}\underset{T\rightarrow\infty}{\lim}\frac{X'\xi}{T}
  \]
\qed
\end{notes}


\begin{frame}
  \frametitle{Erreurs non centrées, II}

  \begin{prop}\label{prop:s2:non-zero-mean-error}
    Supposons que $\mathbb E\left[\varepsilon\right] = \xi \neq 0$, alors~:
    \begin{enumerate}

    \item $\mathbb E\left[ s^2 \right] = \epsvar + \frac{\xi'M\xi}{T-K}$
    \item $s^2 \plim \epsvar + \underset{T\rightarrow\infty}{\lim} \frac{\xi'M\xi}{T}$

    \end{enumerate}
    où $M = I - X(X'X)^{-1}X'$.
  \end{prop}

  \bigskip

  \begin{itemize}

  \item  $\xi'M\xi = \xi'\xi - \xi'X(X'X)^{-1}X'\xi\geq 0$, car $M$ est semi-définie positive.\newline

  \item L'estimateur est sans biais ssi $\xi=0$.\newline

  \item $s^2$ sur-estime la variance des erreurs.

  \end{itemize}

\end{frame}


\begin{notes}
  \textbf{Preuve de la proposition \ref{prop:s2:non-zero-mean-error}.} Supposons que $\varepsilon = \nu + \xi$ où $\nu$ est un vecteur gaussien d'espérance nulle et de variance $\sigma_{\varepsilon}^2I_T$. Il vient~:
  \[
    \begin{split}
      s^2 &= \frac{\varepsilon'M\varepsilon}{T-K}\\
          &= \frac{\left(\nu+\xi\right)'M\left(\nu+\xi\right)}{T-K}\\
          &= \frac{\nu'M\nu}{T-K} + \frac{\xi'M\xi}{T-K} + 2\frac{\nu'M\xi}{T-K}
    \end{split}
  \]
  En prenant l'espérance, on obtient directement (puisque $\nu$ est centré et $\xi$ déterministe)~:
  \[
    \mathbb E\left[ s^2 \right] = \epsvar + \frac{\xi'M\xi}{T-K}
  \]
  On a aussi~:
  \[
    s^2 \plim \epsvar + \underset{T\rightarrow\infty}{\lim}\frac{\xi'M\xi}{T} + 2\underset{T\rightarrow\infty}{\text{plim}}\frac{\nu'M\xi}{T}
  \]
  Si $\nicefrac{\xi'M\xi}{T}$ converge vers un nombre fini, ce que nous supposons, alors le dernier terme doit être nul. En effet, nous avons~:
  \[
    \mathbb E\left[ \frac{\nu'M\xi}{T} \right] = 0
  \]
  et
  \[
    \mathbb V\left[ \frac{\nu'M\xi}{T} \right] = \epsvar \frac{\xi'M\xi}{T^2} \underset{T\rightarrow\infty}{\longrightarrow} 0
    \]
    car la convergence vers un nombre fini de $\nicefrac{\xi'M\xi}{T}$ implique la convergence vers 0 de $\nicefrac{\xi'M\xi}{T^2}$ quand $T$ tend vers l'infini. Ainsi $\nicefrac{\nu'M\xi}{T}$ converge bien en probabilité vers 0.\qed

    \bigskip

    \textbf{Conclusion.} En toute généralité, si les erreurs ne sont pas centrées, il n'est pas possible d'obtenir une estimation sans biais des paramètres. La convergence en probabilité de $\hat{\mathbf b}$ vers $\beta$, même si l'estimateur est biaisé, est assurée si et seulement si la condition $\underset{T\rightarrow\infty}{\lim}\frac{X'\xi}{T}=0$ est satisfaite. La présence d'erreurs non centrées n'affecte pas seulement les estimateurs mais aussi les tests présentés dans le \href{https://le-mans.adjemian.eu/econometrics/chapitre-1.pdf}{chapitre I}.

\end{notes}


\begin{frame}
  \frametitle{Erreurs non centrées, III}

  \begin{prop}\label{prop:nonzero-mean-error-nobiais}
    On considère le DGP et le modèle empirique partitionnés suivants~:
    \[
      \mathbf y = X_1\beta_1 + X_2\beta_2 + \varepsilon
    \]
    \[
      \mathbf y = X_1\mathbf b_1 + X_2\mathbf b_2 + \epsilon
    \]
    avec $\begin{pmatrix}X_1 & X_2\end{pmatrix}=X$, $X_1$ une
    matrice $T\times K_1$, $X_2$ une
    matrice $T\times K_2$, $K = K_1+K_2$. Le DGP vérifie l'ensemble
    des conditions idéales assurant les bonnes propriétés de
    l'estimateur des MCO, sauf l'espérance des erreurs qui satisfait $\mathbb E \left[ \varepsilon \right] = X_1\gamma $
    où $\gamma$ est un vecteur de paramètres $K_1\times 1$
    (l'espérance de $\varepsilon$ est une combinaison linéaire des
    colonnes de $X_1$). Alors $\hat{\mathbf b}_2$ est un estimateur
    sans biais et convergent de $\beta_2$, $s^2$ est un estimateur
    sans biais et convergent de $\epsvar$,
    et $\mathbb E\left[ \hat{\mathbf b}_1 \right] = \beta_1+\gamma$.
  \end{prop}

\end{frame}


\begin{frame}
  \frametitle{Erreurs non centrées, IV}

  \begin{itemize}

  \item La portée de la proposition \ref{prop:nonzero-mean-error-nobiais} peut sembler relativement limitée\dots\newline

  \item Mais~: Si le modèle contient une constante (disons $X_1$ se réduit à une colonne de 1) et si $\mathbf E[\varepsilon_i] = \mathbf E[\varepsilon_j] = c$ pour tout $(i,j)\in\{1,\dots,T\}^2$ alors l'estimation des paramètres de pentes (les paramètres associés aux variables explicatives non constantes) est sans biais et convergente.\newline

  \item[$\Rightarrow$] La non nullité des erreurs n'est pas un problème si le modèle contient une constante.\newline

  \item[\dbend] Il n'est alors pas possible d'identifier la constante et l'espérance des erreurs.

  \end{itemize}

\end{frame}


\begin{notes}

  \textbf{Preuve de la proposition \ref{prop:nonzero-mean-error-nobiais}.} On sait que~:
  \[
    \mathbb E\left[ \hat{\mathbf b} \right] = \beta + (X'X)^{-1}X'\xi
  \]
  En substituant l'expression de $\xi$~:
  \[
    \mathbb E\left[ \hat{\mathbf b} \right] = \beta + (X'X)^{-1}X'X_1\gamma
  \]
  En reprenant la partition~:
  \[
    \mathbb E\left[ \hat{\mathbf b} \right] =
    \begin{pmatrix}
      \beta_1\\ \beta_2
    \end{pmatrix}
    +
    \begin{pmatrix}
      X_1'X_1 & X_1'X_2\\
      X_2'X_1 & X_2'X_2
    \end{pmatrix}^{-1}
    \begin{pmatrix}
      X_1'X_1\\
      X_2'X_1
    \end{pmatrix}\gamma
  \]
  On peut montrer que~:
  \[
\begin{pmatrix}
      X_1'X_1 & X_1'X_2\\
      X_2'X_1 & X_2'X_2
    \end{pmatrix}^{-1}
    \begin{pmatrix}
      X_1'X_1\\
      X_2'X_1
    \end{pmatrix} =
    \begin{pmatrix}
      I\\
      0
    \end{pmatrix}
  \]
  En effet~:
  \[
    \begin{pmatrix}
      X_1'X_1 & X_1'X_2\\
      X_2'X_1 & X_2'X_2
    \end{pmatrix}^{-1}
    \begin{pmatrix}
      X_1'X_1\\
      X_2'X_1
    \end{pmatrix} =
    \begin{pmatrix}
      A\\
      B
    \end{pmatrix}
  \]
  s'écrit de façon équivalente~:
  \[
\begin{pmatrix}
      X_1'X_1 & X_1'X_2\\
      X_2'X_1 & X_2'X_2
\end{pmatrix}
\begin{pmatrix}
      A\\
      B
    \end{pmatrix} =
    \begin{pmatrix}
      X_1'X_1\\
      X_2'X_1
    \end{pmatrix}
  \]
  soit~:
  \[
    \begin{cases}
      X_1'X_1 A +X_1'X_2 B &= X_1'X_1\\
      X_2'X_1 A +X_2'X_2 B &= X_2'X_1
    \end{cases}
  \]
  on doit donc avoir $A=I$ et $B=0$. L'espérance del'estimateur est donc~:
  \[
    \mathbb E\left[ \hat{\mathbf b} \right] =
    \begin{pmatrix}
      \beta_1 + \gamma\\ \beta_2
    \end{pmatrix}
  \]
  L'estimateur $\hat{\mathbf b}_2$ est aussi convergent~:
  \[
    \begin{split}
      \underset{T\rightarrow\infty}{\text{plim}} \hat{\mathbf b} &= \beta
    +
    \underset{T\rightarrow\infty}{\lim} (X'X)^{-1}X'X_1\gamma
                                                                   + \underset{T\rightarrow\infty}{\text{plim}} (X'X)^{-1}X'\nu\\
                                                                 &= \beta
                                                                   +
                                                                   \begin{pmatrix}
                                                                     \gamma I\\
                                                                     0
                                                                   \end{pmatrix}
                                                                   + Q^{-1}\underset{T\rightarrow\infty}{\text{plim}} \frac{X'\nu}{T}\\
                                                                 &=
                                                                   \begin{pmatrix}
                                                                     \beta_1+\gamma\\
                                                                     \beta_2
                                                                   \end{pmatrix}
    \end{split}
  \]
  car $\nicefrac{X'\nu}{T}$ converge en probabilité vers 0. On montre facilement que le biais de $s^2$ est nul. En effet, le biais (voir la proposition \ref{prop:s2:non-zero-mean-error}) est proportionnel à~:
  \[
    \xi'M\xi = \gamma'X_1'M X_1\gamma
  \]
Puisque $MX=0$, a fortiori on doit avoir $MX_1=0$ et donc un biais nul. On montre tout aussi facilement que cet estimateur est convergent.\qed
\end{notes}


\begin{frame}
  \frametitle{Mauvaise spécification du modèle empirique, I}

  \begin{itemize}

  \item Le modèle empirique peut être différent du modèle générateur des données. Par exemple, si des variables explicatives sont omises.\newline

  \item Supposons que le modèle générateur des données soit~:
    \[
      \mathbf y = X_1\beta_1 + X_2\beta_2 + \varepsilon
    \]
    où $\varepsilon$ est un vecteur aléatoire normal $T\times 1$ d'espérance nulle et de variance $\sigma_{\varepsilon}^2I_T$, $\mathbf y$ un vecteur $T\times 1$, $X_1$ et $X_2$ respectivement des matrices déterministes $T\times K_1$ et $T\times K_2$, $\beta_1$ et $\beta_2$ respectivement des vecteurs de paramètres $K_1\times 1$ et $K_2\times 1$.\newline

  \item Le modèle empirique est~:
    \[
      \mathbf y = X_1\mathbf b_1 + \epsilon
    \]
    $\Rightarrow$ les variables $X_2$ sont omises.

  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Mauvaise spécification du modèle empirique, II}

  \begin{itemize}

  \item Il n'est évidemment pas possible d'estimer $\beta_2$...\newline

  \item Est-il possible d'estimer $\beta_1$~? $\sigma_{\varepsilon}^2$~?\newline

  \item A priori non, car les variables omises contaminent les erreurs
    ($\epsilon$) qui n'ont plus les bonnes propriétés.\newline

  \item En comparant les deux modèles, on comprend
    que $\epsilon = \varepsilon + X_2\beta_2$. Les propriétés
    de $\hat{\mathbf b}_1$ ou $s^2$ doivent dépendre
    de $X_2\beta_2$...

  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Mauvaise spécification du modèle empirique, III}

  \begin{prop}\label{prop:mispecification}
    Soit le modèle générateur des données~:
    \[
      \mathbf y = X_1\beta_1 + X_2\beta_2 + \varepsilon
    \]
    satisfaisant toutes les conditions idéales. On estime le modèle~:
    \[
      \mathbf y = X_1\mathbf b_1 + \epsilon
    \]
    par les MCO. On note $\hat{\mathbf b}_1$ et $s^2$ les estimateurs de $\mathbf b_1$ et $\sigma_{\varepsilon}^2$, on a~:
    \begin{enumerate}

    \item $\mathbb E\left[ \mathbf b_1 \right] = \beta_1 + (X_1'X_1)^{-1}X_1'X_2\beta_2$,
    \item $\hat{\mathbf b}_1 \plim \beta_1 + \underset{T\rightarrow\infty}{\lim}\left(\frac{X_1'X_1}{T}\right)^{-1}\underset{T\rightarrow\infty}{\lim}\frac{X_1'X_2}{T}\beta_2$
    \item $\mathbb E\left[s^2\right] = \sigma_{\varepsilon}^2  + \frac{\beta_2'X_2'M_1X_2\beta_2}{T-K}$
    \item $s^2 \plim \sigma_{\varepsilon}^2 + \underset{T\rightarrow\infty}{\lim}\frac{\beta_2'X_2'M_1X_2\beta_2}{T}$
    \end{enumerate}
    avec $M_1 = I-X_1(X_1'X_1)^{-1}X_1'$.
  \end{prop}

\end{frame}


\begin{frame}
  \frametitle{Mauvaise spécification du modèle empirique, IV}

  \begin{itemize}

  \item Comme $X_2$ est de plein rang colonne, $\nexists$ $\beta_2\neq 0$ tel que $X_2\beta_2=0$.\newline

  \item Les estimateurs ne sont pas biaisés si $\beta_2=0$.\newline

  \item Les estimateurs ne sont pas biaisés si $X_1'X_2 = 0$.\newline

  \item[$\Rightarrow$] Si les variables omises ne sont pas corrélées avec les variables incluses dans le modèle empirique, alors les estimateurs ne sont pas biaisés.\newline

  \item Dans ce cas $X'X$ est une matrice bloc diagonale avec $X_1'X_1$ et $X_2'X_2$ le long de la diagonale. Ainsi~:
    \[
      \hat{\mathbf b} =
      \begin{pmatrix}
        (X_1'X_1)^{-1} & 0 \\
        0 & (X_2'X_2)^{-1}
      \end{pmatrix}
      \begin{pmatrix}
        X_1'\mathbf y\\
        X_2'\mathbf y
      \end{pmatrix}
      =
      \begin{pmatrix}
        (X_1'X_1)^{-1}X_1'\mathbf y\\
        (X_2'X_2)^{-1}X_2'\mathbf y\\
      \end{pmatrix}
      =
      \begin{pmatrix}
        \hat{\mathbf b}_1\\
        \hat{\mathbf b}_2
      \end{pmatrix}
    \]
    Les estimations du modèle mal spécifié et du modèle bien spécifié sont identiques.
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Non normalité des erreurs, I}

  \begin{itemize}

  \item On suppose que $\mathbb E\left[ \varepsilon_t \right] = 0$, $\mathbb V\left[ \varepsilon_t \right] = \epsvar$ et $\varepsilon_t\perp\varepsilon_s$ ($s\neq t$) pour tout $t=1,\dots,T$, mais que $\varepsilon$ n'est pas normalement distribué.\newline

  \item $\hat{\mathbf b}$ et $s^2$ sont toujours sans biais, le théorème de Gauss-Markov reste valide (puisque sa démonstration ne fait pas appel à la normalité des erreurs).\newline

  \item Mais  $\hat{\mathbf b}$ n'est plus normalement distribué, $s^2$ ne suit plus une loi du khi-deux.\newline

  \item Les tests développés dans le \href{https://le-mans.adjemian.eu/econometrics/chapitre-1.pdf}{chapitre I} ne sont plus valides.\newline

  \item Néamoins, un théorème de la limite centrale peut être utilisé pour établir la normalité asymptotique de $\hat{\mathbf b}$, et pour obtenir les distributions asymptotiques des tests présentés dans le \href{https://le-mans.adjemian.eu/econometrics/chapitre-1.pdf}{chapitre I}.

  \end{itemize}

\end{frame}



\begin{frame}
  \frametitle{Théorème central limite (version simple)}

  Soit $X_1, X_2, \ldots, X_n$ une suite de variables aléatoires indépendantes et identiquement distribuées (iid) avec une espérance $\mu$ et une variance $\sigma^2$ finies.

  \medskip

  \textbf{Enoncé :} Lorsque $n$ tend vers l'infini, la moyenne empirique $\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i$ converge en distribution vers une loi normale :

  \[
  \bar{X} \xrightarrow{d} \mathcal{N}(\mu, \frac{\sigma^2}{n}) \quad \text{ou} \quad \sqrt{n}(\bar{X} - \mu) \xrightarrow{d} \mathcal{N}(0, \sigma^2)
  \]

  \medskip

  \textbf{Interprétation :} Peu importe la distribution initiale des $X_i$, la distribution de la moyenne $\bar{X}$ approche celle d'une loi normale à mesure que le nombre d'observations augmente.

\end{frame}

\begin{notes}

  \bigskip

\begin{theorem}[TCL i.i.d. via transformée de Laplace]\label{thm:tcl}
Soient $(X_i)_{i\ge1}$ i.i.d.\ avec $\mathbb E[X_i]=\mu<\infty$, $\mathbb V(X_i)=\sigma^2<\infty$
et supposons qu'il existe $t_0>0$ tel que $M_{X_1}(t)=\mathbb E(e^{tX_1})<\infty$ pour $|t|<t_0$.
Notons $S_n=\sum_{i=1}^n X_i$ et $Z_n=(S_n-n\mu)/(\sigma\sqrt n)$, on a $Z_n\Rightarrow\mathcal N(0,1)$.
\end{theorem}

\bigskip

\textbf{Preuve du théorème \ref{thm:tcl}.} Notons $K(t)=\log M_{X_1}(t)$. Comme $M_{X_1}$ existe près de $0$
et $\mathbb E [X_1]=\mu$, $\mathbb V [X_1]=\sigma^2$, on a
\[
K(t)=\mu t+\tfrac12\sigma^2t^2+o(t^2)\quad(t\to0).
\]
La fonction génératrice des moments  de $Z_n$ est, pour $|t|$ assez petit,
\[
M_{Z_n}(t)=\Big(e^{-t\mu/(\sigma\sqrt n)}\,M_{X_1}\!\big(\tfrac{t}{\sigma\sqrt n}\big)\Big)^n,
\]
d'où
\[
\log M_{Z_n}(t)=n\Big(K\!\big(\tfrac{t}{\sigma\sqrt n}\big)-\tfrac{t\mu}{\sigma\sqrt n}\Big)
= n\Big(\tfrac12\sigma^2 \tfrac{t^2}{\sigma^2 n}+o(1/n)\Big)=\tfrac12 t^2+o(1).
\]
Ainsi $M_{Z_n}(t)\to e^{t^2/2}$ lorsque $n\to\infty$ pour $t$ dans un
voisinage de $0$ et on reconnaît la transformée de Laplace de la loi
normale centrée réduite.  Par unicité de la loi donnée par la
fonction génératrice des moments, $Z_n\Rightarrow\mathcal N(0,1)$.\newline

Si la fonction génératrice des moments n'est pas finie, une preuve
plus générale consiste à utiliser la fonction caractéristique.

\end{notes}

\begin{notes}

  \begin{center}
    \begin{tabular}{c}
      \\
      \Huge{\textsc{Références}}\\
      \\
    \end{tabular}
  \end{center}

  \bigskip

  \nocite{Green2017}

  \nocite{Schmidt1976}

  \printbibliography

\end{notes}


\end{document}


% Local Variables:
% ispell-check-comments: exclusive
% ispell-local-dictionary: "french"
% TeX-master: t
% End:
