\synctex=1

\documentclass[10pt]{beamer}

\usepackage[T1]{fontenc}
\usepackage{etex}
\usepackage{fourier-orns}
\usepackage{ccicons}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsbsy}
\usepackage{amsopn}
\usepackage{amscd}
\usepackage{amsxtra}
\usepackage{amsthm}
\usepackage{float}
\usepackage{color, colortbl}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage[nice]{nicefrac}
\usepackage{setspace}
\usepackage{ragged2e}
\usepackage{listings}
\usepackage{algorithms/algorithm}
\usepackage{algorithms/algorithmic}
\usepackage{tikz,pgfplots,pgfplotstable}
\pgfplotsset{compat=1.18}%newest}
\usetikzlibrary{patterns, arrows, decorations.pathreplacing, decorations.markings, calc}
\pgfplotsset{plot coordinates/math parser=false}
\usetikzlibrary{external}
\tikzexternalize[prefix=figures/]
\newlength\figureheight
\newlength\figurewidth
\usepackage{cancel}
\usepackage{tikz-qtree}
\usepackage{dcolumn}
\usepackage{adjustbox}
\usepackage{environ}
\usepackage[cal=boondox]{mathalfa}
\usepackage{manfnt}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  filecolor=black,
  urlcolor=blue,
}
\usepackage{venndiagram}
\usepackage{subcaption}
\usepackage{centernot}

\usepackage[backend=biber,style=bwl-FU,natbib=true,doi=false,isbn=false,url=false,eprint=false]{biblatex}%bwl-FU
\addbibresource{econometrics.bib}

\makeatletter
\@ifclassloaded{beamer}{
\usefonttheme[onlymath]{serif}
\uselanguage{French}
\languagepath{French}
% Git hash
\usepackage{xstring}
\usepackage{catchfile}
\immediate\write18{git rev-parse HEAD > git.hash}
\CatchFileDef{\HEAD}{git.hash}{\endlinechar=-1}
\newcommand{\gitrevision}{\StrLeft{\HEAD}{7}}
}{}
\makeatother

\newcommand{\trace}{\mathrm{tr}}
\newcommand{\vect}{\mathrm{vec}}
\newcommand{\tracarg}[1]{\mathrm{tr}\left\{#1\right\}}
\newcommand{\vectarg}[1]{\mathrm{vec}\left(#1\right)}
\newcommand{\vecth}[1]{\mathrm{vech}\left(#1\right)}
\newcommand{\iid}[2]{\mathrm{iid}\left(#1,#2\right)}
\newcommand{\normal}[2]{\mathcal N\left(#1,#2\right)}
\newcommand{\sample}{\mathcal Y_T}
\newcommand{\samplet}[1]{\mathcal Y_{#1}}
\newcommand{\slidetitle}[1]{\fancyhead[L]{\textsc{#1}}}

\newcommand{\R}{{\mathbb R}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\binomial}[2]{\begin{pmatrix} #1 \\ #2 \end{pmatrix}}
\newcommand{\bigO}[1]{\mathcal O \left(#1\right)}
\newcommand{\red}{\color{red}}
\newcommand{\blue}{\color{blue}}
\newcommand{\plim}{\overset{\text{proba}}{\underset{T\rightarrow\infty}{\longrightarrow}}}
\newcommand{\epsvar}{\sigma_{\varepsilon}^2}


\newcommand\gauss[2]{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))} % Gaussian probability density function.

\renewcommand{\qedsymbol}{C.Q.F.D.}

\newcolumntype{d}{D{.}{.}{-1}}
\definecolor{gray}{gray}{0.9}
\newcolumntype{g}{>{\columncolor{gray}}c}


\makeatletter
\@ifclassloaded{beamer}{\setbeamertemplate{theorems}[numbered]{}}{}
\makeatother

\theoremstyle{plain}

\makeatletter
\@ifclassloaded{beamer}{
\setbeamertemplate{footline}{
  {\hfill\vspace*{1pt}\href{http://creativecommons.org/licenses/by-sa/3.0/legalcode}{\ccbysa}\hspace{.1cm}
    \href{https://github.com/stepan-a/econometrics/blob/\HEAD/cours/chapitre-2.tex}{\gitrevision}\enspace--\enspace\today\enspace
  }}

\makeatother


\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{blocks}[rounded][shadow=true]
\setbeamertemplate{caption}[numbered]

\NewEnviron{notes}{\justifying\footnotesize\begin{spacing}{1.0}\BODY\vfill\pagebreak\end{spacing}}

\newenvironment{exercise}[1]
{\bgroup \small\begin{block}{Ex. #1}}
  {\end{block}\egroup}

\newenvironment{defn}[1]
{\bgroup \small\begin{block}{Définition. #1}}
  {\end{block}\egroup}

\newenvironment{exemple}[1]
{\bgroup \small\begin{block}{Exemple. #1}}
  {\end{block}\egroup}
}{}

\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollaire}

%\usepgfplotslibrary{external}
%\tikzexternalize


\begin{document}

\title{Économétrie\\\small{Violations des conditions idéales}}
\author[S. Adjemian]{Stéphane Adjemian}
\institute{\texttt{stephane.adjemian@univ-lemans.fr}}
\date{Septembre 2024}

\begin{frame}
  \titlepage{}
\end{frame}


\begin{frame}
  \frametitle{Les conditions idéales}

  Dans le chapitre précédent, nous avons supposé que~:\newline

  \medskip

  \begin{itemize}

  \item Les erreurs sont centrées, \textit{ie} $\mathbb E[\varepsilon]=0$.\newline

  \item Le modèle empirique est bien spécifié.\newline

  \item Les erreurs sont normalement distruées.\newline

  \item La matrice $X$ est de plein rang colonne.\newline

  \item Les erreurs sont homoscédastiques et non autocorrélées, \textit{ie}  $\mathbb V[\varepsilon] = \epsvar I_T$.\newline

  \end{itemize}

  \medskip

  $\Rightarrow$ Comment se comporte l'estimateur des MCO si ces conditions ne sont pas réunies~?

\end{frame}


\begin{frame}
  \frametitle{Erreurs non centrées, I}

  \begin{prop}\label{prop:bhat:non-zero-mean-error}
    Supposons que $\mathbb E\left[\varepsilon\right] = \xi \neq 0$, alors~:
    \begin{enumerate}

    \item $\mathbb E\left[ \hat{\mathbf b} \right] = \beta + (X'X)^{-1}X'\xi$
    \item $\hat{\mathbf b} \plim \beta + Q^{-1}\underset{T\rightarrow\infty}{\lim} \frac{X'\xi}{T}$

    \end{enumerate}
  \end{prop}

  \bigskip

  \begin{itemize}

  \item Les colonnes de $X$ sont linéairement indépendantes $\Rightarrow$$X'\xi=0$ ssi $\xi=0$.\newline

  \item $\hat{\mathbf b}$ est un estimateur sans biais de $\beta$ ssi $\xi=0$.\newline

  \item \textbf{Remarque:} A priori $\xi$ est différent de $\mathbf 1 \triangleq \begin{pmatrix}1 & 1 & \dots & 1\end{pmatrix}'$.

  \end{itemize}

\end{frame}


\begin{notes}
  \textbf{Preuve de la proposition \ref{prop:bhat:non-zero-mean-error}.} Supposons que $\varepsilon = \nu + \xi$ où $\nu$ est un vecteur gaussien d'espérance nulle et de variance $\sigma_{\varepsilon}^2I_T$. En substituant le DGP dans l'espression de l'estimateur des MCO, il vient~:
  \[
    \begin{split}
      \hat{\mathbf b}_T &= (X'X)^{-1}X'\left( X\beta + \xi + \nu\right)\\
      &= \beta + (X'X)^{-1}X'\xi + (X'X)^{-1}X'\nu
    \end{split}
  \]
  En prenant l'espérance, on obtient directement~:
  \[
    \mathbb E\left[ \hat{\mathbf b} \right] = \beta + (X'X)^{-1}X'\xi
  \]
  puisque le vecteur $\nu$ est d'espérance nulle. On a aussi~:
  \[
    \hat{\mathbf b} \plim \beta + Q^{-1}\underset{T\rightarrow\infty}{\lim}\frac{X'\xi}{T} + Q^{-1}\underset{T\rightarrow\infty}{\text{plim}}\frac{X'\nu}{T}
  \]
  On peut montrer que le dernier terme est nul. En effet $\mathbb E\left[\frac{X'\nu}{T}\right]=0$ et
  \[
    \mathbb V \left[ \frac{X'\nu}{T} \right] = \frac{\epsvar}{T}\frac{X'X}{T} \underset{T\rightarrow\infty}{\longrightarrow} 0\times Q = 0
  \]
  Ainsi nous avons bien~:
  \[
    \hat{\mathbf b} \plim \beta + Q^{-1}\underset{T\rightarrow\infty}{\lim}\frac{X'\xi}{T}
  \]
\qed
\end{notes}


\begin{frame}
  \frametitle{Erreurs non centrées, II}

  \begin{prop}\label{prop:s2:non-zero-mean-error}
    Supposons que $\mathbb E\left[\varepsilon\right] = \xi \neq 0$, alors~:
    \begin{enumerate}

    \item $\mathbb E\left[ s^2 \right] = \epsvar + \frac{\xi'M\xi}{T-K}$
    \item $s^2 \plim \epsvar + \underset{T\rightarrow\infty}{\lim} \frac{\xi'M\xi}{T}$

    \end{enumerate}
    où $M = I - X(X'X)^{-1}X'$.
  \end{prop}

  \bigskip

  \begin{itemize}

  \item  $\xi'M\xi = \xi'\xi - \xi'X(X'X)^{-1}X'\xi\geq 0$, car $M$ est semi-définie positive.\newline

  \item L'estimateur est sans biais ssi $\xi=0$.\newline

  \item $s^2$ sur-estime la variance des erreurs.

  \end{itemize}

\end{frame}


\begin{notes}
  \textbf{Preuve de la proposition \ref{prop:s2:non-zero-mean-error}.} Supposons que $\varepsilon = \nu + \xi$ où $\nu$ est un vecteur gaussien d'espérance nulle et de variance $\sigma_{\varepsilon}^2I_T$. Il vient~:
  \[
    \begin{split}
      s^2 &= \frac{\varepsilon'M\varepsilon}{T-K}\\
          &= \frac{\left(\nu+\xi\right)'M\left(\nu+\xi\right)}{T-K}\\
          &= \frac{\nu'M\nu}{T-K} + \frac{\xi'M\xi}{T-K} + 2\frac{\nu'M\xi}{T-K}
    \end{split}
  \]
  En prenant l'espérance, on obtient directement (puisque $\nu$ est centré et $\xi$ déterministe)~:
  \[
    \mathbb E\left[ s^2 \right] = \epsvar + \frac{\xi'M\xi}{T-K}
  \]
  On a aussi~:
  \[
    s^2 \plim \epsvar + \underset{T\rightarrow\infty}{\lim}\frac{\xi'M\xi}{T} + 2\underset{T\rightarrow\infty}{\text{plim}}\frac{\nu'M\xi}{T}
  \]
  Si $\nicefrac{\xi'M\xi}{T}$ converge vers un nombre fini, ce que nous supposons, alors le dernier terme doit être nul. En effet, nous avons~:
  \[
    \mathbb E\left[ \frac{\nu'M\xi}{T} \right] = 0
  \]
  et
  \[
    \mathbb V\left[ \frac{\nu'M\xi}{T} \right] = \epsvar \frac{\xi'M\xi}{T^2} \underset{T\rightarrow\infty}{\longrightarrow} 0
    \]
    car la convergence vers un nombre fini de $\nicefrac{\xi'M\xi}{T}$ implique la convergence vers 0 de $\nicefrac{\xi'M\xi}{T^2}$ quand $T$ tend vers l'infini. Ainsi $\nicefrac{\nu'M\xi}{T}$ converge bien en probabilité vers 0.\qed

    \bigskip

    \textbf{Conclusion.} En toute généralité, si les erreurs ne sont pas centrées, il n'est pas possible d'obtenir une estimation sans biais des paramètres. La convergence en probabilité de $\hat{\mathbf b}$ vers $\beta$, même si l'estimateur est biaisé, est assurée si et seulement si la condition $\underset{T\rightarrow\infty}{\lim}\frac{X'\xi}{T}=0$ est satisfaite. La présence d'erreurs non centrées n'affecte pas seulement les estimateurs mais aussi les tests présentés dans le \href{https://le-mans.adjemian.eu/econometrics/chapitre-1.pdf}{chapitre I}.

\end{notes}


\begin{frame}
  \frametitle{Erreurs non centrées, III}

  \begin{prop}\label{prop:nonzero-mean-error-nobiais}
    On considère le DGP et le modèle empirique partitionnés suivants~:
    \[
      \mathbf y = X_1\beta_1 + X_2\beta_2 + \varepsilon
    \]
    \[
      \mathbf y = X_1\mathbf b_1 + X_2\mathbf b_2 + \epsilon
    \]
    avec $\begin{pmatrix}X_1 & X_2\end{pmatrix}=X$, $X_1$ une
    matrice $T\times K_1$, $X_2$ une
    matrice $T\times K_2$, $K = K_1+K_2$. Le DGP vérifie l'ensemble
    des conditions idéales assurant les bonnes propriétés de
    l'estimateur des MCO, sauf l'espérance des erreurs qui satisfait $\mathbb E \left[ \varepsilon \right] = X_1\gamma $
    où $\gamma$ est un vecteur de paramètres $K_1\times 1$
    (l'espérance de $\varepsilon$ est une combinaison linéaire des
    colonnes de $X_1$). Alors $\hat{\mathbf b}_2$ est un estimateur
    sans biais et convergent de $\beta_2$, $s^2$ est un estimateur
    sans biais et convergent de $\epsvar$,
    et $\mathbb E\left[ \hat{\mathbf b}_1 \right] = \beta_1+\gamma$.
  \end{prop}

\end{frame}


\begin{frame}
  \frametitle{Erreurs non centrées, IV}

  \begin{itemize}

  \item La portée de la proposition \ref{prop:nonzero-mean-error-nobiais} peut sembler relativement limitée\dots\newline

  \item Mais~: Si le modèle contient une constante (disons $X_1$ se réduit à une colonne de 1) et si $\mathbf E[\varepsilon_i] = \mathbf E[\varepsilon_j] = c$ pour tout $(i,j)\in\{1,\dots,T\}^2$ alors l'estimation des paramètres de pentes (les paramètres associés aux variables explicatives non constantes) est sans biais et convergente.\newline

  \item[$\Rightarrow$] La non nullité des erreurs n'est pas un problème si le modèle contient une constante.\newline

  \item[\dbend] Il n'est alors pas possible d'identifier la constante et l'espérance des erreurs.

  \end{itemize}

\end{frame}


\begin{notes}

  \textbf{Preuve de la proposition \ref{prop:nonzero-mean-error-nobiais}.} On sait que~:
  \[
    \mathbb E\left[ \hat{\mathbf b} \right] = \beta + (X'X)^{-1}X'\xi
  \]
  En substituant l'expression de $\xi$~:
  \[
    \mathbb E\left[ \hat{\mathbf b} \right] = \beta + (X'X)^{-1}X'X_1\gamma
  \]
  En reprenant la partition~:
  \[
    \mathbb E\left[ \hat{\mathbf b} \right] =
    \begin{pmatrix}
      \beta_1\\ \beta_2
    \end{pmatrix}
    +
    \begin{pmatrix}
      X_1'X_1 & X_1'X_2\\
      X_2'X_1 & X_2'X_2
    \end{pmatrix}^{-1}
    \begin{pmatrix}
      X_1'X_1\\
      X_2'X_1
    \end{pmatrix}\gamma
  \]
  On peut montrer que~:
  \[
\begin{pmatrix}
      X_1'X_1 & X_1'X_2\\
      X_2'X_1 & X_2'X_2
    \end{pmatrix}^{-1}
    \begin{pmatrix}
      X_1'X_1\\
      X_2'X_1
    \end{pmatrix} =
    \begin{pmatrix}
      I\\
      0
    \end{pmatrix}
  \]
  En effet~:
  \[
    \begin{pmatrix}
      X_1'X_1 & X_1'X_2\\
      X_2'X_1 & X_2'X_2
    \end{pmatrix}^{-1}
    \begin{pmatrix}
      X_1'X_1\\
      X_2'X_1
    \end{pmatrix} =
    \begin{pmatrix}
      A\\
      B
    \end{pmatrix}
  \]
  s'écrit de façon équivalente~:
  \[
\begin{pmatrix}
      X_1'X_1 & X_1'X_2\\
      X_2'X_1 & X_2'X_2
\end{pmatrix}
\begin{pmatrix}
      A\\
      B
    \end{pmatrix} =
    \begin{pmatrix}
      X_1'X_1\\
      X_2'X_1
    \end{pmatrix}
  \]
  soit~:
  \[
    \begin{cases}
      X_1'X_1 A +X_1'X_2 B &= X_1'X_1\\
      X_2'X_1 A +X_2'X_2 B &= X_2'X_1
    \end{cases}
  \]
  on doit donc avoir $A=I$ et $B=0$. L'espérance del'estimateur est donc~:
  \[
    \mathbb E\left[ \hat{\mathbf b} \right] =
    \begin{pmatrix}
      \beta_1 + \gamma\\ \beta_2
    \end{pmatrix}
  \]
  L'estimateur $\hat{\mathbf b}_2$ est aussi convergent~:
  \[
    \begin{split}
      \underset{T\rightarrow\infty}{\text{plim}} \hat{\mathbf b} &= \beta
    +
    \underset{T\rightarrow\infty}{\lim} (X'X)^{-1}X'X_1\gamma
                                                                   + \underset{T\rightarrow\infty}{\text{plim}} (X'X)^{-1}X'\nu\\
                                                                 &= \beta
                                                                   +
                                                                   \begin{pmatrix}
                                                                     \gamma I\\
                                                                     0
                                                                   \end{pmatrix}
                                                                   + Q^{-1}\underset{T\rightarrow\infty}{\text{plim}} \frac{X'\nu}{T}\\
                                                                 &=
                                                                   \begin{pmatrix}
                                                                     \beta_1+\gamma\\
                                                                     \beta_2
                                                                   \end{pmatrix}
    \end{split}
  \]
  car $\nicefrac{X'\nu}{T}$ converge en probabilité vers 0. On montre facilement que le biais de $s^2$ est nul. En effet, le biais (voir la proposition \ref{prop:s2:non-zero-mean-error}) est proportionnel à~:
  \[
    \xi'M\xi = \gamma'X_1'M X_1\gamma
  \]
Puisque $MX=0$, a fortiori on doit avoir $MX_1=0$ et donc un biais nul. On montre tout aussi facilement que cet estimateur est convergent.\qed
\end{notes}


\begin{frame}
  \frametitle{Mauvaise spécification du modèle empirique, I}

  \begin{itemize}

  \item Le modèle empirique peut être différent du modèle générateur des données. Par exemple, si des variables explicatives sont omises.\newline

  \item Supposons que le modèle générateur des données soit~:
    \[
      \mathbf y = X_1\beta_1 + X_2\beta_2 + \varepsilon
    \]
    où $\varepsilon$ est un vecteur aléatoire normal $T\times 1$ d'espérance nulle et de variance $\sigma_{\varepsilon}^2I_T$, $\mathbf y$ un vecteur $T\times 1$, $X_1$ et $X_2$ respectivement des matrices déterministes $T\times K_1$ et $T\times K_2$, $\beta_1$ et $\beta_2$ respectivement des vecteurs de paramètres $K_1\times 1$ et $K_2\times 1$.\newline

  \item Le modèle empirique est~:
    \[
      \mathbf y = X_1\mathbf b_1 + \epsilon
    \]
    $\Rightarrow$ les variables $X_2$ sont omises.

  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Mauvaise spécification du modèle empirique, II}

  \begin{itemize}

  \item Il n'est évidemment pas possible d'estimer $\beta_2$...\newline

  \item Est-il possible d'estimer $\beta_1$~? $\sigma_{\varepsilon}^2$~?\newline

  \item A priori non, car les variables omises contaminent les erreurs
    ($\epsilon$) qui n'ont plus les bonnes propriétés.\newline

  \item En comparant les deux modèles, on comprend
    que $\epsilon = \varepsilon + X_2\beta_2$. Les propriétés
    de $\hat{\mathbf b}_1$ ou $s^2$ doivent dépendre
    de $X_2\beta_2$...

  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Mauvaise spécification du modèle empirique, III}

  \begin{prop}\label{prop:mispecification}
    Soit le modèle générateur des données~:
    \[
      \mathbf y = X_1\beta_1 + X_2\beta_2 + \varepsilon
    \]
    satisfaisant toutes les conditions idéales. On estime le modèle~:
    \[
      \mathbf y = X_1\mathbf b_1 + \epsilon
    \]
    par les MCO. On note $\hat{\mathbf b}_1$ et $s^2$ les estimateurs de $\mathbf b_1$ et $\sigma_{\varepsilon}^2$, on a~:
    \begin{enumerate}

    \item $\mathbb E\left[ \mathbf b_1 \right] = \beta_1 + (X_1'X_1)^{-1}X_1'X_2\beta_2$,
    \item $\hat{\mathbf b}_1 \plim \beta_1 + \underset{T\rightarrow\infty}{\lim}\left(\frac{X_1'X_1}{T}\right)^{-1}\underset{T\rightarrow\infty}{\lim}\frac{X_1'X_2}{T}\beta_2$
    \item $\mathbb E\left[s^2\right] = \sigma_{\varepsilon}^2  + \frac{\beta_2'X_2'M_1X_2\beta_2}{T-K}$
    \item $s^2 \plim \sigma_{\varepsilon}^2 + \underset{T\rightarrow\infty}{\lim}\frac{\beta_2'X_2'M_1X_2\beta_2}{T}$
    \end{enumerate}
    avec $M_1 = I-X_1(X_1'X_1)^{-1}X_1'$.
  \end{prop}

\end{frame}


\begin{frame}
  \frametitle{Mauvaise spécification du modèle empirique, IV}

  \begin{itemize}

  \item Comme $X_2$ est de plein rang colonne, $\nexists$ $\beta_2\neq 0$ tel que $X_2\beta_2=0$.\newline

  \item Les estimateurs ne sont pas biaisés si $\beta_2=0$.\newline

  \item Les estimateurs ne sont pas biaisés si $X_1'X_2 = 0$.\newline

  \item[$\Rightarrow$] Si les variables omises ne sont pas corrélées avec les variables incluses dans le modèle empirique, alors les estimateurs ne sont pas biaisés.\newline

  \item Dans ce cas $X'X$ est une matrice bloc diagonale avec $X_1'X_1$ et $X_2'X_2$ le long de la diagonale. Ainsi~:
    \[
      \hat{\mathbf b} =
      \begin{pmatrix}
        (X_1'X_1)^{-1} & 0 \\
        0 & (X_2'X_2)^{-1}
      \end{pmatrix}
      \begin{pmatrix}
        X_1'\mathbf y\\
        X_2'\mathbf y
      \end{pmatrix}
      =
      \begin{pmatrix}
        (X_1'X_1)^{-1}X_1'\mathbf y\\
        (X_2'X_2)^{-1}X_2'\mathbf y\\
      \end{pmatrix}
      =
      \begin{pmatrix}
        \hat{\mathbf b}_1\\
        \hat{\mathbf b}_2
      \end{pmatrix}
    \]
    Les estimations du modèle mal spécifié et du modèle bien spécifié sont identiques.
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Non normalité des erreurs, I}

  \begin{itemize}

  \item On suppose que $\mathbb E\left[ \varepsilon_t \right] = 0$, $\mathbb V\left[ \varepsilon_t \right] = \epsvar$ et $\varepsilon_t\perp\varepsilon_s$ ($s\neq t$) pour tout $t=1,\dots,T$, mais que $\varepsilon$ n'est pas normalement distribué.\newline

  \item $\hat{\mathbf b}$ et $s^2$ sont toujours sans biais, le théorème de Gauss-Markov reste valide (puisque sa démonstration ne fait pas appel à la normalité des erreurs).\newline

  \item Mais  $\hat{\mathbf b}$ n'est plus normalement distribué, $s^2$ ne suit plus une loi du khi-deux.\newline

  \item Les tests développés dans le \href{https://le-mans.adjemian.eu/econometrics/chapitre-1.pdf}{chapitre I} ne sont plus valides.\newline

  \item Néamoins, un théorème de la limite centrale peut être utilisé pour établir la normalité asymptotique de $\hat{\mathbf b}$, et pour obtenir les distributions asymptotiques des tests présentés dans le \href{https://le-mans.adjemian.eu/econometrics/chapitre-1.pdf}{chapitre I}.

  \end{itemize}

\end{frame}



\begin{frame}
  \frametitle{Théorème central limite (version simple)}

  Soit $X_1, X_2, \ldots, X_n$ une suite de variables aléatoires indépendantes et identiquement distribuées (iid) avec une espérance $\mu$ et une variance $\sigma^2$ finies.

  \medskip

  \textbf{Enoncé :} Lorsque $n$ tend vers l'infini, la moyenne empirique $\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i$ converge en distribution vers une loi normale :

  \[
  \bar{X} \xrightarrow{d} \mathcal{N}(\mu, \frac{\sigma^2}{n}) \quad \text{ou} \quad \sqrt{n}(\bar{X} - \mu) \xrightarrow{d} \mathcal{N}(0, \sigma^2)
  \]

  \medskip

  \textbf{Interprétation :} Peu importe la distribution initiale des $X_i$, la distribution de la moyenne $\bar{X}$ approche celle d'une loi normale à mesure que le nombre d'observations augmente.

\end{frame}

\begin{notes}

  \bigskip

\begin{theorem}[TCL i.i.d. via transformée de Laplace]\label{thm:tcl}
Soient $(X_i)_{i\ge1}$ i.i.d.\ avec $\mathbb E[X_i]=\mu<\infty$, $\mathbb V(X_i)=\sigma^2<\infty$
et supposons qu'il existe $t_0>0$ tel que $M_{X_1}(t)=\mathbb E(e^{tX_1})<\infty$ pour $|t|<t_0$.
Notons $S_n=\sum_{i=1}^n X_i$ et $Z_n=(S_n-n\mu)/(\sigma\sqrt n)$, on a $Z_n\Rightarrow\mathcal N(0,1)$.
\end{theorem}

\bigskip

\textbf{Preuve du théorème \ref{thm:tcl}.} Notons $K(t)=\log M_{X_1}(t)$. Comme $M_{X_1}$ existe près de $0$
et $\mathbb E [X_1]=\mu$, $\mathbb V [X_1]=\sigma^2$, on a
\[
K(t)=\mu t+\tfrac12\sigma^2t^2+o(t^2)\quad(t\to0).
\]
La fonction génératrice des moments  de $Z_n$ est, pour $|t|$ assez petit,
\[
M_{Z_n}(t)=\Big(e^{-t\mu/(\sigma\sqrt n)}\,M_{X_1}\!\big(\tfrac{t}{\sigma\sqrt n}\big)\Big)^n,
\]
d'où
\[
\log M_{Z_n}(t)=n\Big(K\!\big(\tfrac{t}{\sigma\sqrt n}\big)-\tfrac{t\mu}{\sigma\sqrt n}\Big)
= n\Big(\tfrac12\sigma^2 \tfrac{t^2}{\sigma^2 n}+o(1/n)\Big)=\tfrac12 t^2+o(1).
\]
Ainsi $M_{Z_n}(t)\to e^{t^2/2}$ lorsque $n\to\infty$ pour $t$ dans un
voisinage de $0$ et on reconnaît la transformée de Laplace de la loi
normale centrée réduite.  Par unicité de la loi donnée par la
fonction génératrice des moments, $Z_n\Rightarrow\mathcal N(0,1)$.\newline

Si la fonction génératrice des moments n'est pas finie, une preuve
plus générale consiste à utiliser la fonction caractéristique.

\end{notes}


\begin{frame}
  \frametitle{Multicolinéarité, I}

  \begin{itemize}

  \item Si la matrice $X$ n'est pas de plein rang colonne, l'estimateur des MCO n'est pas défini.\newline

  \item[$\Rightarrow$] Retirer une ou des variables explicative(s).\newline

  \item Un problème plus intéressant est le cas de la colinéarité approchée des régresseurs.\newline

  \item La matrice $X'X$ est alors proche d'être singulière (au moins une valeur propre est proche de zéro) :\newline

    \begin{itemize}
    \item Problème numérique dans le calcul de $(X'X)^{-1}$\newline
    \item Faible précision de l'estimateur des MCO\newline
    \end{itemize}

  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Multicolinéarité, II}

  \begin{itemize}

  \item Considérons le cas d'un modèle linéaire avec deux variables explicatives. Le DGP est~:
    \[
      \mathbf y = X\beta + \varepsilon
    \]
    et le modèle empirique~:
    \[
      \mathbf y = X \mathbf{b} + \epsilon
    \]
    où $X$ est une matrice $T\times 2$, $y$ une
    vecteur $T\times 1$, $\varepsilon$ un vecteur
    aléatoire $T\times 1$ tel que $\mathbb E[\varepsilon] = 0$
    et $\mathbb V[\varepsilon] = \sigma^2I_T$, $\beta$ un
    vecteur $2\times 1$.\newline

  \item On suppose que la matrice des régresseurs est telle que~:
    \[
      \frac{1}{T} X'X =
      \begin{pmatrix}
        1 & \rho \\
        \rho & 1
      \end{pmatrix}
    \]
    où $\rho\in]-1,1[$ est la corrélation entre les deux régresseurs.\newline

    \end{itemize}

  \end{frame}


\begin{frame}
  \frametitle{Multicolinéarité, III}

  \begin{itemize}


  \item La variance de l'estimateur des MCO est alors~:
    \[
      \mathbb V[\hat{\mathbf{b}}] = \frac{\sigma^2}{T}
      \begin{pmatrix}
        1 & \rho\\
        \rho & 1
      \end{pmatrix}^{-1}
      = \frac{\sigma^2}{T(1-\rho^2)}
      \begin{pmatrix}
        -\rho & 1\\
        1 & -\rho
      \end{pmatrix}
    \]

    \medskip

  \item La variance est d'autant plus grande, l'estimateur est d'autant moins précis, que la corrélation est proche de 1.\newline

  \item Comme toujours, elle est aussi d'autant plus grande que l'échantillon est petit $\rightarrow$ la colinéarité approchée peut être compensée par la taille de l'échantillon~!\newline

  \item Si les variables sont trés corrélées, il faut beaucoup plus d'observations pour distinquer les effets respectifs des deux variables.\newline

  \item On note que $\rho$, qui caractérise le degré de colinéarité, n'affecte pas l'espérance de l'estimateur ($\beta$).

  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Erreurs non sphériques, I}

  \begin{itemize}

  \item Supposons que, dans un modèle linéaire, le vecteur des erreus centrées soit tel que~:
    \[
      \mathbb V[\varepsilon] = \sigma_{\varepsilon}^2\Omega \neq \sigma_{\varepsilon}^2 I_T
    \]

  \item Le long de la diagonale d'$\Omega$ les éléments ne sont pas constants $\rightarrow$ hétéroscédasticité.\newline

  \item Les éléments en dehors de la diagonale peuvent être non nuls $\rightarrow$ autocorrélation\newline

  \item Pas de conséquences sur le biais, mais sur la variance

  \end{itemize}


  \begin{prop}\label{prop:bhat:non-spherical-errors-1}
    L'estimateur des MCO, $\hat{\textbf{b}}$, est sans biais. Si $\lim_{T\rightarrow\infty}\frac{X'\Omega X}{T}$ est finie alors $\hat{\textbf{b}}$ est un estimateur convergent de $\beta$.
  \end{prop}

\end{frame}


\begin{notes}

  \textbf{Preuve de la proposition \ref{prop:bhat:non-spherical-errors-1}.} On a~:
  \[
    \hat{\textbf{b}} = \beta + (X'X)^{-1}X'\varepsilon
  \]
  En appliqaunt l'opérateur espérance, il vient directement~:
  \[
    \mathbf E[\hat{\textbf{b}}] = \beta
  \]
  La non sphéricité des erreurs ne biaise pas l'estimateur des MCO. La limite en probabilité de l'estimateur est~:
  \[
    \begin{split}
      \hat{\textbf{b}} &\plim \beta + \lim_{T\rightarrow\infty}\frac{X'X}{T}\underset{T\rightarrow\infty}{\text{plim}}\frac{X'\varepsilon}{T}\\
      &\plim \beta + Q\times \underset{T\rightarrow\infty}{\text{plim}}\frac{X'\varepsilon}{T}
    \end{split}
  \]
  où, sous les hypothèses usuelles, $Q$ est une matrice de plein rang. On a~:
  \[
    \mathbb E\left[\frac{X'\varepsilon}{T}\right] = 0
  \]
  et
  \[
    \begin{split}
      \mathbb V\left[\frac{X'\varepsilon}{T}\right] &= \frac{X'\mathbb V[\varepsilon]X}{T^2}\\
                                                    &= \sigma_{\varepsilon}^2\frac{X'\Omega X}{T^2} \underset{T\rightarrow\infty}{\longrightarrow} 0
    \end{split}
  \]
  car par hypothèse $\frac{X'\Omega X}{T}$ tend vers une constante finie. Ainsi, la limite en probabilité de $\frac{X'\varepsilon}{T}$ est 0.\qed
\end{notes}


\begin{frame}
  \frametitle{Erreurs non sphériques, II}

  \begin{prop}\label{prop:bhat:non-spherical-errors-2}
    La variance de l'estimateur des MCO, $\hat{\textbf{b}}$, est~:
    \[
      \mathbb V[\hat{\textbf{b}}] = \sigma_{\varepsilon}^2 (X'X)^{-1}X'\Omega X (X'X)^{-1}
    \]
  \end{prop}

  \begin{prop}\label{prop:bhat:non-spherical-errors-3}
    $s^2=\frac{SSE}{T-K}$ est un estimateur biaisé et non convergent de $\sigma_{\varepsilon}^2$,
  \end{prop}

  \bigskip

  \begin{itemize}

  \item La variance de l'estimateur des MCO est différente de $\sigma_{\varepsilon}^2 (X'X)^{-1}$ dès lors que $\Omega\neq I_T$... Elle peut être plus grande ou plus petite (la différence peut-être semi définie positive, semi définie négative ou autre)

  \end{itemize}

\end{frame}


\begin{notes}

  \textbf{Preuve de la proposition \ref{prop:bhat:non-spherical-errors-2}.} Par définition de la variance et de $\hat{\textbf{b}}$, on a~:
  \[
    \begin{split}
      \mathbb V\left[\hat{\textbf{b}}\right] &= \mathbb E \left[(X'X)^{-1}X'\varepsilon\varepsilon' X (X'X-1) \right]\\
                                             &= (X'X)^{-1}X'\mathbb E[\varepsilon\varepsilon']X(X'X)^{-1}\\
      &= \sigma_{\varepsilon}^2(X'X)^{-1}X'\Omega X (X'X)^{-1}
    \end{split}
  \]
  \qed

  \bigskip

  \textbf{Preuve de la proposition \ref{prop:bhat:non-spherical-errors-3}.} La somme des carrés des résidus, exprimée en fonction des erreurs, est donnée par~:
  \[
    SSE = \varepsilon' M \varepsilon
  \]
  où $M= I_T - X (X'X)^{-1}X'$ et $P = X(X'X)^{-1}X'$ est la matrice
  de projection orthogonale sur l'espace engendré par les colonnes
  de $X$. L'espérance de la somme des carrés des résidus est donc~:
  \[
    \begin{split}
      \mathbb E[SSE] &= \mathbb E \left[\varepsilon' M \varepsilon \right]\\
                     &= \text{trace} \mathbb E [M\varepsilon'\varepsilon]\\
                     &= \text{trace} M \mathbb E [\varepsilon'\varepsilon]\\
                     &= \sigma_{\varepsilon}^{2}\text{trace} M\Omega
    \end{split}
  \]
  qui a priori est différent de $\sigma_{\varepsilon}^2(T-K)$, à cause de la présence de $\Omega$ sous la trace. Ainsi la limite en probabilité de $s¨2$ est a priori différente de $\sigma_{\varepsilon}^2$.\qed

\end{notes}


\begin{frame}
  \frametitle{Estimateur des MCG, I}

  \begin{itemize}

  \item Est-il possible de transformer les données, pour se ramener dans une situation où toutes les conditions idéales sont satisfaites et où l'estimateur des MCO est BLUE~?\newline

  \item[$\Rightarrow$] L'estimateur des Moindres Carrés Généralisés (MCG).\newline

  \item Puisque $\Omega$ est une matrice définie positive, $\Omega^{-1}$ est aussi une matrice définie positive.\newline

  \item On sait donc qu'il existe une matrice de plein rang $\Lambda$ telle que $\Omega^{-1} = \Lambda'\Lambda$.\newline

  \end{itemize}


  \begin{prop}\label{prop:mcg}
    Le modèle transformé $\Lambda \mathbf y = \Lambda X \beta + \Lambda\varepsilon$ satisfait les conditions idéales si $\lim_{T\rightarrow\infty}\frac{X'\Omega^{-1}X}{T}$ est finie (une matrice symétrique définie positive).
  \end{prop}


\end{frame}


\begin{notes}

  \textbf{Preuve de la proposition \ref{prop:mcg}.} Puisque $\Lambda$ est une matrice déterministe non singulière, le vecteur aléatoire $\Lambda\varepsilon$ est normalement distribué, son espérance est~:
  \[
    \mathbb E [\Lambda\varepsilon] = 0
  \]
  et sa variance~:
  \[
    \begin{split}
      \mathbb V[\Lambda\varepsilon] &= \Lambda \mathbb V[\varepsilon] \Lambda'\\
      &= \sigma_{\varepsilon}^2 \Lambda \Omega \Lambda'
    \end{split}
  \]
  En prémultipliant la variance par $\Lambda'$, on obtient~:
  \[
    \sigma_{\varepsilon}^2\Lambda'\Lambda \Omega\lambda' = \sigma_{\varepsilon}^2\Omega^{-1}\Omega \Lambda' = \sigma_{\varepsilon}^2\Lambda'
  \]
  On en déduit donc que $\Lambda \Omega \Lambda' = I_T$. Ainsi $\mathbb V [\Lambda\varepsilon] = \sigma_{\varepsilon}^2I_T$. Les erreurs du modèle transformé sont donc normalement distribuées, centrées et sphériques. Il nous reste à vérifier le comportement des régresseurs dans le modèle transformé. Puisque $\Lambda$ et $X$ sont des matrices déterministes, les régresseurs dans le modèle transformé sont déterministes. On a~:
  \[
    \lim_{T\rightarrow\infty}\frac{(\Lambda X)'\Lambda X}{T} = \lim_{T\rightarrow\infty}\frac{X'\Lambda'\Lambda X}{T} = \lim_{T\rightarrow\infty}\frac{X'\Omega^{-1} X}{T}
  \]
  qui par hypothèse est une matrice (finie) définie positive.\qed
\end{notes}


\begin{frame}
  \frametitle{Estimateur des MCG, II}

  \begin{itemize}

  \item Puisque le modèle transformé satisfait toutes les conditions idéales, on peut utiliser l'estimateur des MCO en régressant $\Lambda \mathbf y$ sur $\Lambda X$ pour obtenir un estimateur BLUE de $\beta$ $\rightarrow$estimateur MCG\newline

  \item On suppose implicitement ici que la matrice $\Omega$, et donc $\Lambda$, est connue.\newline

  \end{itemize}


  \begin{prop}\label{prop:mcg:def}
    L'estimateur des Moindres Carrés Généralisés de $\beta$ est défini par~:
    \[
      \tilde{\textbf{b}} = (X'\Omega^{-1}X)^{-1}X'\Omega^{-1}\mathbf y
    \]
    Il s'agit d'un estimateur BLUE et convergent de $\beta$.
  \end{prop}


\end{frame}


\begin{notes}

  \textbf{Preuve de la proposition \ref{prop:mcg:def}.} Par définition de l'estimateur des MCO (sur les données transformées) on a~:

  \[
    \begin{split}
      \tilde{\textbf{b}} &= \left( (\Lambda X)'\lambda X\right)^{-1}(\Lambda X)'\Lambda \mathbf y\\
                         &= \left( X'\Lambda' \Lambda X \right)^{-1}X'\Lambda'\Lambda \mathbf y\\
                         &= \left( X'\Omega^{-1}X \right)^{-1}X'\Omega^{-1}\mathbf y
    \end{split}
  \]
  Puisque le modèle transformé vérifie toutes les conditions idéales, il est BLUE et convergent.\qed

  \bigskip

  Nous verrons plus loin que l'estimateur MCG de $\beta$ est identique à l'estimateur du maximum de vraisemblance.

\end{notes}


\begin{frame}
  \frametitle{Estimateur des MCG, III}


  \begin{prop}\label{prop:mcg:variance}
    La variance de l'estimateur des Moindres Carrés Généralisés de $\beta$ est~:
    \[
      \mathbb V \left[\tilde{\textbf{b}}\right] = \sigma_{\varepsilon}^2(X'\Omega^{-1}X)^{-1}
    \]
  \end{prop}

  \bigskip

  \begin{prop}\label{prop:mcg:s2}
    Un estimateur non biaisé et convergent de $\sigma_{\varepsilon}^2$ est~:
    \[
      \tilde s^2 = \frac{\left(y-X\tilde{\textbf{b}}\right)'\Omega^{-1}\left(y-X\tilde{\textbf{b}}\right)}{T-K}
    \]
  \end{prop}

\end{frame}


\begin{notes}

  \textbf{Preuve de la proposition \ref{prop:mcg:variance}.} En reprenant la formule de la variance de l'estimateur des MCO et en l'appliquant sur les données transformées~:
  \[
    \begin{split}
      \mathbb V \left[\tilde{\textbf{b}}\right] &= \sigma_{\varepsilon}^2\left((\Lambda X)'\Lambda X  \right)^{-1}\\
                                                &= \sigma_{\varepsilon}^2\left(X'\Lambda'\Lambda X  \right)^{-1}\\
                                                &= \sigma_{\varepsilon}^2\left(X'\Omega^{-1} X \right)^{-1}\\
    \end{split}
  \]
  \qed

  \bigskip

  \textbf{Comparaison de $\mathbb V [\tilde{\textbf{b}}]$
    et $\mathbb V [\hat{\textbf{b}}]$.} On peut montrer que la
  variance de la variance de l'estimateur des MCO est plus grande que
  la variance de l'estimateur des MCG, au sens
  où $\mathbb V [\hat{\textbf{b}}]-\mathbb V [\tilde{\textbf{b}}]$ est
  une matrice définie positive. Nous savons déjà
  que $\tilde{\textbf{b}}$ est BLUE est que donc sa variance doit être
  plus petite, mais nous envisagons une preuve plus directe sans
  s'appuer sur le théorème de Gauss-Markov. Soit le modèle~:

  \[
\mathbf y=X\beta+\varepsilon,\quad \mathbb E[\varepsilon]=0,\quad \mathbb V[\varepsilon]=\Sigma = \sigma_{\varepsilon}^2\succ 0.
\]
Posons~:
\[
  A = (X'X)^{-1}X'
\]
et
\[
  G = (X'\Sigma^{-1}X)^{-1}X'\Sigma^{-1}
\]
On peut alors écrire les variance des deux estimateurs comme~:
\[
\mathbb V[\hat{\textbf{b}}] = A\Sigma A'
\]
et
\[
\mathbb V[\tilde{\textbf{b}}] = G\Sigma G'
\]
Posons \(D=A-G\). On a \(AX=GX=I_K\), donc \(DX=0\). La différence des variances est~:
\[
  \begin{split}
    \mathbb V[\hat{\textbf{b}}]-\mathbb V[\tilde{\textbf{b}}] &= A\Sigma A' - G\Sigma G'\\
&= (G+D)\Sigma(G+D)' - G\Sigma G'\\
&= D\Sigma D' + G\Sigma D' + D\Sigma G'
  \end{split}
\]
Or
\[
G\Sigma D' = (X'\Sigma^{-1}X)^{-1}X'D' = 0
\]
de même pour sa transposée $D\Sigma G'$. Ainsi
\[
  \mathbb V[\hat{\textbf{b}}]-\mathbb V[\tilde{\textbf{b}}] = D\Sigma D \succeq 0
\]
est une matrice définie positive. La différence est nulle si et
seulement si $D=0$, c'est-à-dire si $\Sigma$ est proportionnelle à une
matrice identité. L'estimateur MCG est donc bien plus précis que
l'estimateur des MCO quand les résidus sont non sphériques.

\bigskip

\textbf{Preuve de la proposition \ref{prop:mcg:s2}.} On procède comme
pour la preuve de la proposition \ref{prop:mcg:variance} en partant de
la définition de $\tilde{s}^2$ dans le chapitre
\href{https://le-mans.adjemian.eu/econometrics/chapitre-1.pdf}{1} et
en considérant les données transformées.\qed

\end{notes}


\begin{frame}
  \frametitle{Estimateur des MCG, IV}

  \begin{itemize}

  \item Pour le reste, nous pouvons faire les tests comme dans le chapitre \href{https://le-mans.adjemian.eu/econometrics/chapitre-1.pdf}{I}.\newline

  \item L'estimateur des MCG de $\beta$ est normalement distribué~:
    \[
      \tilde{\mathbf b} \sim \mathcal N\left(\beta, \sigma_{\varepsilon}^2\left(X'\Omega^{-1} X \right)^{-1} \right)
    \]
    \medskip

  \item $\tilde{s}^2$ suit un khi-2~:
    \[
      \frac{(T-K)\tilde{s}^2}{\sigma_{\varepsilon}^2} \sim \chi^2(T-K)
    \]
    \medskip

  \item Les variables aléatoires $\tilde{\mathbf b}$ et $\tilde{s}^2$ sont indépendantes.
  \end{itemize}


\end{frame}





\begin{notes}

  \begin{center}
    \begin{tabular}{c}
      \\
      \Huge{\textsc{Références}}\\
      \\
    \end{tabular}
  \end{center}

  \bigskip

  \nocite{Green2017}

  \nocite{Schmidt1976}

  \printbibliography

\end{notes}


\end{document}


% Local Variables:
% ispell-check-comments: exclusive
% ispell-local-dictionary: "french"
% TeX-master: t
% End:
