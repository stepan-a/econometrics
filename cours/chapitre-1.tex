\synctex=1

\documentclass[10pt]{beamer}

\usepackage[T1]{fontenc}
\usepackage{etex}
\usepackage{fourier-orns}
\usepackage{ccicons}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsbsy}
\usepackage{amsopn}
\usepackage{amscd}
\usepackage{amsxtra}
\usepackage{amsthm}
\usepackage{float}
\usepackage{color, colortbl}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage[nice]{nicefrac}
\usepackage{setspace}
\usepackage{ragged2e}
\usepackage{listings}
\usepackage{algorithms/algorithm}
\usepackage{algorithms/algorithmic}
\usepackage{tikz,pgfplots,pgfplotstable}
\pgfplotsset{compat=1.18}%newest}
\usetikzlibrary{patterns, arrows, decorations.pathreplacing, decorations.markings, calc}
\pgfplotsset{plot coordinates/math parser=false}
\usetikzlibrary{external}
\tikzexternalize[prefix=figures/]
\newlength\figureheight
\newlength\figurewidth
\usepackage{cancel}
\usepackage{tikz-qtree}
\usepackage{dcolumn}
\usepackage{adjustbox}
\usepackage{environ}
\usepackage[cal=boondox]{mathalfa}
\usepackage{manfnt}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  filecolor=black,
  urlcolor=black,
}
\usepackage{venndiagram}
\usepackage{subcaption}
\usepackage{centernot}

\usepackage[backend=biber,style=bwl-FU,natbib=true,doi=false,isbn=false,url=false,eprint=false]{biblatex}%bwl-FU
\addbibresource{econometrics.bib}

\makeatletter
\@ifclassloaded{beamer}{
\usefonttheme[onlymath]{serif}
\uselanguage{French}
\languagepath{French}
% Git hash
\usepackage{xstring}
\usepackage{catchfile}
\immediate\write18{git rev-parse HEAD > git.hash}
\CatchFileDef{\HEAD}{git.hash}{\endlinechar=-1}
\newcommand{\gitrevision}{\StrLeft{\HEAD}{7}}
}{}
\makeatother

\newcommand{\trace}{\mathrm{tr}}
\newcommand{\vect}{\mathrm{vec}}
\newcommand{\tracarg}[1]{\mathrm{tr}\left\{#1\right\}}
\newcommand{\vectarg}[1]{\mathrm{vec}\left(#1\right)}
\newcommand{\vecth}[1]{\mathrm{vech}\left(#1\right)}
\newcommand{\iid}[2]{\mathrm{iid}\left(#1,#2\right)}
\newcommand{\normal}[2]{\mathcal N\left(#1,#2\right)}
\newcommand{\sample}{\mathcal Y_T}
\newcommand{\samplet}[1]{\mathcal Y_{#1}}
\newcommand{\slidetitle}[1]{\fancyhead[L]{\textsc{#1}}}

\newcommand{\R}{{\mathbb R}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\binomial}[2]{\begin{pmatrix} #1 \\ #2 \end{pmatrix}}
\newcommand{\bigO}[1]{\mathcal O \left(#1\right)}
\newcommand{\red}{\color{red}}
\newcommand{\blue}{\color{blue}}

\newcommand\gauss[2]{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))} % Gaussian probability density function.

\renewcommand{\qedsymbol}{C.Q.F.D.}

\newcolumntype{d}{D{.}{.}{-1}}
\definecolor{gray}{gray}{0.9}
\newcolumntype{g}{>{\columncolor{gray}}c}


\makeatletter
\@ifclassloaded{beamer}{\setbeamertemplate{theorems}[numbered]{}}{}
\makeatother

\theoremstyle{plain}

\makeatletter
\@ifclassloaded{beamer}{
\setbeamertemplate{footline}{
  {\hfill\vspace*{1pt}\href{http://creativecommons.org/licenses/by-sa/3.0/legalcode}{\ccbysa}\hspace{.1cm}
    \href{https://github.com/stepan-a/economic-calculus/blob/\HEAD/cours/chapitre-1.tex}{\gitrevision}\enspace--\enspace\today\enspace
  }}

\makeatother


\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{blocks}[rounded][shadow=true]
\setbeamertemplate{caption}[numbered]

\NewEnviron{notes}{\justifying\footnotesize\begin{spacing}{1.0}\BODY\vfill\pagebreak\end{spacing}}

\newenvironment{exercise}[1]
{\bgroup \small\begin{block}{Ex. #1}}
  {\end{block}\egroup}

\newenvironment{defn}[1]
{\bgroup \small\begin{block}{Définition. #1}}
  {\end{block}\egroup}

\newenvironment{exemple}[1]
{\bgroup \small\begin{block}{Exemple. #1}}
  {\end{block}\egroup}
}{}

\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollaire}

%\usepgfplotslibrary{external}
%\tikzexternalize


\begin{document}

\title{Économétrie\\\small{Les MCO quand tout va bien}}
\author[S. Adjemian]{Stéphane Adjemian}
\institute{\texttt{stephane.adjemian@univ-lemans.fr}}
\date{Septembre 2024}

\begin{frame}
  \titlepage{}
\end{frame}

\section{DGP et modèle empirique}

\begin{frame}
  \frametitle{Le modèle de la nature}
  \framesubtitle{Un modèle linéaire}

  \bigskip

  On suppose que les données ($y$) sont générées par le modèle suivant~:
  \[
    y_t = \beta_1x_{1,t} + \beta_2x_{2,t} + \dots + \beta_Kx_{K,t} + \varepsilon_t
  \]

  \bigskip

  \begin{itemize}
  
  \item $y$ est la variable endogène (ou expliquée).\newline

  \item $x_{k}$, $k=1,\ldots,K$, sont les variables exogènes (ou explicatives).\newline

  \item $\varepsilon_t$ est une variable aléatoire, elle rend compte de ce qui ne peut être expliqué par les variables $x_k$.\newline

  \item La nature nous donne un échantillon $\{y_t,x_{1,t},\ldots,x_{K,t}\}_{t=1}^T$ ($T$ est le nombre d'observations).\newline

  \item Les $K$ variables exogènes peuvent être déterministes (pour simplifier) ou aléatoires.\newline

  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Le modèle de la nature}
  \framesubtitle{Variables exogènes déterministes ou aléatoires}

  \begin{itemize}

  \item Les variables exogènes sont déterministes $\Leftrightarrow$
    Lorsque l'économètre s'adresse à la nature afin d'obtenir un
    nouvel échantillon, elle lui renvoit toujours les mêmes valeurs
    pour les variables exogènes.\newline

  \item Dans le cas de variables exogènes non
    stochastiques, $\varepsilon$ est la seule source d'aléa.\newline

  \item[$\Rightarrow$] La loi de $y$ est directement déduite de celle de $\varepsilon$.\newline

  \item Cette hypothèse simplifie grandement l'étude des propriétés de
    l'estimateur des Moindres Carrés Ordinaires, mais dans certaines
    circonstances elle est beaucoup trop forte (voire n'a aucun sens
    comme dans le cas des modèles dynamiques).\newline

  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Le modèle de la nature}
  \framesubtitle{Variables exogènes déterministes ou aléatoires}

  \begin{figure}
    \centering
    \begin{subfigure}{0.4\textwidth}
      \scalebox{.3}{
    \input{images/chapitre-1/sample-nonstochastic-x.tex}}
    \caption{$x$ déterministe.}
    \label{fig:01:a}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.4\textwidth}
    \scalebox{.3}{
    \input{images/chapitre-1/sample-stochastic-x.tex}}
    \caption{$x$ stochastique.}
    \label{fig:01:b}
  \end{subfigure}
  \label{fig:01}
  \caption{Le modèle de la nature est $y_t = x_t + \varepsilon_t$ avec $x_t$ une variable exogène prenant des valeurs dans l'intervalle $[0,10]$ et $\varepsilon_t$ une variable alléatoire gaussienne centrée réduite avec $\mathbb E[\varepsilon_t\varepsilon_s]=0$ si $s\neq t$. Chaque figure représente 100 échantillons de 10 observations. Les codes pour reproduire ces graphiques sont disponibles \href{https://mnemosyne.ithaca.fr/stephane/econometrics/-/blob/\HEAD/cours/codes/chapitre-1/deterministic-versus-stochastic-samples.py}{ici}.}
\end{figure}
\end{frame}


\begin{frame}
  \frametitle{Le modèle de la nature}
  \framesubtitle{Représentation matricielle}

  \begin{itemize}

  \item Ce modèle peut être représenté matriciellement sous la forme :
    \[
      \mathbf y = X\beta + \varepsilon
    \]
    avec $\mathbf y = \left( y_1, y_2, \dots, y_T\right)'$ et $\varepsilon = \left( \varepsilon_1, \varepsilon_2, \dots, \varepsilon_T\right)'$ des vecteurs $T\times 1$, $\beta = \left( \beta_1, \dots, \beta_K \right)'$  un vecteur $K\times 1$ et
    \[
      X =
      \begin{pmatrix}
        x_{1,1} & x_{2,1} & \dots & x_{K,1} \\
        x_{1,2} & x_{2,2} & \dots & x_{K,2} \\
        \vdots  & \vdots  &       & \vdots  \\
        x_{1,T} & x_{2,2} & \dots & x_{K,T} \\
      \end{pmatrix}
    \]
    une matrice $T\times K$.\newline

    \item On notera $\mathbf x_t = (x_{1,t},x_{2,t}, \dots, x_{K,t})$ la t-ième observation pour les exogènes (un vecteur $1\times K$).

  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Le modèle de la nature}
  \framesubtitle{Hypothèses}

  \begin{itemize}

  \item[$\mathcal H_1$] Les variables exogènes sont déterministes.\newline

    \bigskip\bigskip

  \item[$\mathcal H_2$] $X$ est une matrice de rang $K<T$ et vérifie :
    \[
      \lim_{T\rightarrow\infty} \frac{X'X}{T} = Q
    \]
    où $Q$ est une matrice symétrique définie positive.\newline

    \bigskip\bigskip

  \item[$\mathcal H_3$] $\varepsilon$ suit loi normale multivariée d'espérance nulle et de variance $\sigma_{\varepsilon}^2I_T$.
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Le modèle de l'économètre}
  \framesubtitle{Pas de mauvaise spécification}

  \begin{itemize}

  \item On suppose que l'économmètre connaît la forme du modèle de la nature.\newline

    \bigskip

  \item Mais il ne connaît pas les valeurs des paramètres $\beta$ qu'il va chercher à estimer...\newline

    \bigskip

  \item ... En utilisant l'unique échantillon que lui donne la nature.\newline

    \bigskip

  \item Le modèle empirique est donc~:
    \[
      \mathbf y = X\mathbf b + \epsilon
    \]
    où $\mathbf b$ est le vecteur des paramètres du modèle empirique.
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}

\begin{defn}{}
  L'estimateur des MCO de $\mathbf b$ minimise la somme des carrés des résidus:
  \[
    \begin{split}
      \hat{\mathbf b} &= \arg\min_{\mathbf b} \sum_{t=1}^T \epsilon_t^2\\
      &= \arg\min_{\mathbf b} (\mathbf y-X \mathbf b)'(\mathbf y-X\mathbf b)
    \end{split}
  \]
\end{defn}

\bigskip

\begin{itemize}

\item $\epsilon_t^2$ est une mesure de la distance entre l'observation $y_t$ et la prédiction $\mathbf x_t\mathbf b$.\newline

\item Le choix de la distance entre observation et prédiction est
  arbitraire.\newline

\item L'estimateur des MCO minimise la somme des carrés des erreurs de prédiction.\newline

\end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}
  \framesubtitle{Formule}

\begin{theorem}\label{thm:ols}
  L'estimateur des MCO de $\mathbf b$ dans le modèle $\mathbf y=X\mathbf b + \epsilon$ est~:
  \[
    \hat{\mathbf{b}} = \left( X'X \right)^{-1}X'\mathbf y
  \]
\end{theorem}


\bigskip

\textbf{Preuve.} \small{La forme quadratique que nous devons minimiser s'écrit en développant~: $\mathcal S(\mathbf b) = \mathbf y'\mathbf y-\mathbf y'X\mathbf b-\mathbf b'X'\mathbf y+\mathbf b'X'X\mathbf b$, une fonction de $\mathbb R^K$ dans $\mathbb R^+$. En notant que $\mathbf y'X\mathbf b = \mathbf b'X'\mathbf y$ puisque la transposée d'un scalaire est égale au scalaire, on a~: $\mathcal S(\mathbf b) = \mathbf y'\mathbf y-2\mathbf b'X'\mathbf y+\mathbf b'X'X\mathbf b$. En annulant la dérivée de $\mathcal S$ par rapport à $\mathbf b$ on obtient la condition du premier ordre~:
\[
-2X'\mathbf y + 2 X'X \hat{\mathbf b} = 0
\]
soit de façon équivalente~:
\[
\hat{\mathbf{b}} = \left( X'X \right)^{-1}X'\mathbf y
\]
car $X'X$ est de plein rang par $\mathcal H_2$.}\qed
\end{frame}


\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}
  \framesubtitle{Remarques sur le théorème \ref{thm:ols}}

  \begin{itemize}

  \item La condition du premier ordre qui permet d'identifier $\hat{\mathbf b}$ est linéaire car l'objectif est quadratique.\newline

  \item La matrice hessienne de l'objectif est bien définie positive~:
    \[
      \frac{\partial^2\mathcal S}{\partial \mathbf b \partial\mathbf b'} = 2 X'X
    \]

  \item Notons aussi qu'il est possible de réécrire l'objectif de la façon suivante~:
    \[
      \mathcal S(\mathbf b) = \mathcal S(\hat{\mathbf b}) + \left(\mathbf b - \hat{\mathbf b}  \right)' X'X \left(\mathbf b - \hat{\mathbf b}  \right)
    \]
    puisque le dernier terme est toujours positif (car la matrice hessienne $X'X$ est définie positive), on voit directement que la somme des carrés des résidus est minimale en $\hat{\mathbf b}$.

  \end{itemize}

\end{frame}


% EXEMPLE

\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}
  \framesubtitle{Résidus estimés}

  \begin{defn}{}
    Les résidus estimés sont définis par~:
    \[
      \hat{\epsilon}  = \mathbf y - X \hat{\mathbf b}
    \]
  \end{defn}

  \bigskip

  \begin{prop}\label{prop:orthogonal_residuals}
    Les résidus estimés sont orthogonaux aux variables explicatives~:
    \[
    X\hat\epsilon = 0
  \]
  \end{prop}

  \bigskip

  \begin{cor}\label{cor:zero_mean_residuals}
    Si les variables explicatives contiennent une constante alors les résidus somment à zéro.
  \end{cor}

\end{frame}

\begin{notes}
  \begin{itemize}

      \item \textbf{Preuve de la proposition \ref{prop:orthogonal_residuals}.} En partant de la CNO pour $\hat{\mathbf b}$~:
  \[
    -2X'\mathbf y + 2 X'X \hat{\mathbf b} = 0
  \]
  et en factorisant on a directement~:
    \[
      X'(\mathbf y - X \hat{\mathbf b}) = 0
    \]
    soit par définition des résidus estimés~:  $X'\hat\epsilon = 0$ \qed\newline

    \item \textbf{Preuve du corollaire \ref{cor:zero_mean_residuals}.} Supposons, sans perte de généralité, que les éléments de la première colonne de $X$ soit tous égaux à 1. Le premier élément du vecteur $K\times 1$ $X'\hat\epsilon$ est alors~:
    \[
      \begin{pmatrix}
        1 & 1 & \ldots & 1
      \end{pmatrix}
      \begin{pmatrix}
        \hat\epsilon_1\\
        \hat\epsilon_2\\
        \vdots\\
        \hat\epsilon_T
      \end{pmatrix} = \sum_{t=1}^T\hat\epsilon_t
    \]
    qui doit être égal à zéro.\qed\newline

  \item Le corollaire \ref{cor:zero_mean_residuals} implique en particulier que les
      résidus sont nécessairement de moyenne nulle dès lors que le modèle contient
      une constante.

  \end{itemize}
\end{notes}

\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}
  \framesubtitle{Sommes de carrés}

  \begin{defn}{}
    On note $\hat y_t = x_t \hat{\mathbf b}$ le prédicteur de $y_t$, $\bar y = T^{-1}\sum_{t=1}^T y_t$ la moyenne arithmétique, et on définit~:
    \[
      \begin{split}
        SSE &= \hat\epsilon'\hat\epsilon = \sum_{t=1}^T(y_t-\hat y_t)^2\\
        SSR &= \sum_{t=1}^T(\hat y_t-\bar y)^2\\
        SST &= \sum_{t=1}^T(y_t-\bar y)^2
      \end{split}
    \]
  \end{defn}

  \bigskip

  \begin{prop}\label{prop:SST_SSR_SSE}
    Si les variables explicatives contiennent une constante alors~:
    \[
    SST = SSR + SSE
  \]
  \end{prop}

\end{frame}


\begin{notes}
  \begin{itemize}

  \item \textbf{Lemme.} La somme des carrés des résidus vérifie $\hat\epsilon'\hat\epsilon = \mathbf y'\mathbf y - \hat{\mathbf y}'\hat{\mathbf y}$.\newline

    \textbf{Preuve.} Par définition des résidus estimés, on a~:
    \[
      \begin{split}
        \hat\epsilon'\hat\epsilon &= (\mathbf y-X\hat{\mathbf b})'(\mathbf y-X\hat{\mathbf b})\\
                                  &= \mathbf y'\mathbf y - \mathbf y'X\hat{\mathbf b} - \hat{\mathbf b}'X'\mathbf y + \hat{\mathbf b}'X'X\hat{\mathbf b}\\
                                  &= \mathbf y'\mathbf y - 2\hat{\mathbf b}'X'\mathbf y + \hat{\mathbf b}'X'X\hat{\mathbf b}\\
                                  &= \mathbf y'\mathbf y - 2\hat{\mathbf b}'X'X\hat{\mathbf b} + \hat{\mathbf b}'X'X\hat{\mathbf b}\\
                                  &= \mathbf y'\mathbf y - \hat{\mathbf b}'X'X\hat{\mathbf b}\\
                                  &= \mathbf y'\mathbf y - \hat{\mathbf y}'\hat{\mathbf y}
      \end{split}
    \]
où on passe à la troisième égalité en notant qu'un scalaire est égal à sa transposée, puis à la quatrième en utilisant la CNO pour $\hat{\mathbf b}$ qui nous dit que $X'\mathbf y = X'X \hat{\mathbf b}$.\qed\newline

\item \textbf{Preuve de la proposition \ref{prop:SST_SSR_SSE}.} On sait, voir le lemme donné au dessus, que~:
  \[
    \mathbf y'\mathbf y = \hat{\mathbf y}' \hat{\mathbf y} + \hat\epsilon'\hat\epsilon
  \]
  En retranchant $T\bar y^2$ sur les deux membres, il vient~:
  \[
    \mathbf y'\mathbf y - T\bar y^2 = \hat{\mathbf y}' \hat{\mathbf y} - T\bar y^2 + \hat\epsilon'\hat\epsilon
  \]
  Par ailleurs~:
  \[
    \begin{split}
      SST &= \sum_{t=1}^T(y_t-\bar y)^2\\
          &= \sum_{t=1}^Ty_t^2 - 2\bar y\sum_{t=1}^Ty_t + T\bar y^2\\
          &= \mathbf y'\mathbf y - 2T\bar y^2 + T\bar y^2\\
          &= \mathbf y'\mathbf y - T\bar y^2
    \end{split}
  \]
  nous retrouvons donc $SST$ sur le membre de gauche. On a aussi~:
\[
    \begin{split}
      SSR &= \sum_{t=1}^T(\hat y_t-\bar y)^2\\
          &= \sum_{t=1}^T\hat y_t^2 - 2\bar y\sum_{t=1}^T\hat y_t + T\bar y^2\\
    \end{split}
  \]
  En rappelant que  $y_t = \mathbf x_t \hat{\mathbf b} + \hat\epsilon_t = \hat y_t + \hat\epsilon_t$, on a encore~:
  \[
    SSR= \sum_{t=1}^T\hat y_t^2 - 2\bar y\sum_{t=1}^T (y_t-\hat\epsilon_t) + T\bar y^2
  \]
  et puisque les résidus estimés somment à zéro quand $X$ contient une constante~:
  \[
    SSR = \sum_{t=1}^T\hat y_t^2 - 2\bar y\sum_{t=1}^T y_t + T\bar y^2
  \]
  et donc~:
  \[
    SSR = \hat{\mathbf y}'\hat{\mathbf y} - T\bar y^2
  \]
Ainsi, nous avons finalement~:
  \[
    SST = SSR + SSE
  \]
  \qed\newline

  \end{itemize}
\end{notes}


\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}
  \framesubtitle{Coefficient de détermination}

  \begin{defn}{}
    Le coefficient de détermination est~:
    \[
      R^2 = 1 - \frac{SSE}{SST}
    \]
  \end{defn}

  \bigskip

  \begin{prop}\label{prop:r2}
    Si les variables explicatives contiennent une constante alors~:
    \begin{enumerate}
      \item $R^2 = \frac{SSR}{SST}$
      \item $0 \leq R^2 \leq 1$
      \item $\sqrt{R^2} = \mathtt{corr}(y,\hat y)$
    \end{enumerate}
  \end{prop}

\end{frame}


\begin{notes}
  \begin{itemize}
  \item \textbf{Preuve de la proposition \ref{prop:r2}.} \textbf{(1)} En utilisant la proposition \ref{prop:SST_SSR_SSE} on a~:
    \[
      \frac{SSR}{SST} = \frac{SST-SSE}{SST} = 1 - \frac{SSE}{SST}
    \]
    \textbf{(2)} Comme $SSR$ et $SST$ sont des sommes de carrés, le $R^2$ ne peut-être négatif. Comme, pour la même raison, $\frac{SSE}{SST}\geq 0$ le $R^2$ ne peut être supérieur à 1. \textbf{(3)} La corrélation entre $y$ et $\hat y$ est définie par~:
    \[
      \mathtt{corr}(y,\hat y) = \frac{\mathtt{cov(y,\hat y)}}{\sqrt{\mathbb V[y]\mathbb V[\hat y]}}
    \]
    La variance de la variable expliquée est~:
    \[
      \mathbb V [y] = \frac{1}{T}\sum_{t=1}^T(y_t-\bar y)^2 = \frac{SST}{T}
    \]
    La variance de la prédiction, en notant que $\bar y$ est aussi la moyenne de $\hat y$ puisque les résidus estimés somment à zéro, est~:
    \[
      \mathbb V [\hat y] = \frac{1}{T}\sum_{t=1}^T(\hat y_t-\bar y)^2 = \frac{SSR}{T}
    \]
    La covariance entre $y$ et $\hat y$ est~:
    \[
      \mathtt{cov}(y,\hat y) = \frac{1}{T}\sum_{t=1}^T(y_t-\bar y)(\hat y_t- \bar y)
    \]
    En développant sous la somme~:
    \[
      \begin{split}
        \mathtt{cov}(y,\hat y) &= \frac{1}{T}\sum_{t=1}^T y_t\hat y_t - \bar y y_t - \bar y \hat y_t + \bar y^2\\
                               &= \frac{1}{T}\sum_{t=1}^T y_t\hat y_t - \bar y^2
      \end{split}
    \]
    Puisque $y_t = \hat y_t + \hat \epsilon_t$ et $\sum_{t=1}^T\hat \epsilon_t \hat y_t  = \hat{\mathbf y}'\hat\epsilon = \hat{\mathbf b}'X'\hat\epsilon = 0$ car les résidus estimés sont orthogonaux aux variables explicatives, nous avons~:
    \[
      \mathtt{cov}(y,\hat y) = \frac{1}{T}\sum_{t=1}^T\hat y_t^2 - \bar y^2 = \frac{SSR}{T}
    \]
    Notons en passant que la covariance entre $y$ et $\hat y$ est nécessairement positive (c'est heureux). La corrélation est donc~:
    \[
      \mathtt{corr}(y,\hat y) = \sqrt{\frac{SSR}{SST}} = \sqrt{R^2}
    \]
    \qed\newline
  \end{itemize}
\end{notes}


\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}
  \framesubtitle {Remarques sur le  $R^2$}

  \begin{itemize}

  \item Par définition le $R^2$ est toujours plus petit que 1.\newline

  \item Pour que le $R^2$ soit positif il faut que le modèle contienne
    une constante.\newline

  \item Dans ce cas, le $R^2$ mesure la contribution de la variabilité de $x$ à la variance de $y$.\newline

  \item Les prédictions \textit{in-sample} ($\hat{\mathbf y}$) sont d'autant meilleures (proches de $\mathbf y$) que le $R^2$ est proche de 1.\newline

  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}
  \framesubtitle {Remarques sur le  $R^2$ (suite)}


  \begin{itemize}

  \item Un $R^2$ proche de 1 ne garantit pas que le modèle soit \guillemotleft bon\guillemotright.\newline

  \item De bonnes prévisions \textit{in-sample} ne garantissent pas de bonnes prévisions \textit{out-of-sample}.\newline

  \item Le $R^2$ ne préjuge pas de la relation entre $y$ et $x$ (ce n'est pas parce que le $R^2$ est faible que $x$ n'a pas d'effet significatif sur $y$).\newline

  \item Un grand $R^2$ proche de 1 ne veut pas dire que la variable $x$ explique  \guillemotleft bien\guillemotright  la variable $y$. Nous obtiendrions le même  $R^2$ en inversant le modèle empirique $\Rightarrow$ Pas d'interprétation causale.

  \end{itemize}

\end{frame}


\begin{frame}[c]
  \frametitle{Estimateur des Moindres Carrés Ordinaires}

  \begin{itemize}

  \item Échantillons (différents DGP) avec mêmes moments d'ordre 1 et 2.\newline

  \item Même modèle empirique $\Rightarrow$ Estimations et $R^2=0,67$ identiques.

  \end{itemize}

  \begin{figure}
    \centering
    \scalebox{.5}{
      \input{images/chapitre-1/anscombe.tex}}
    \label{fig:02}
    \caption{Les quatre échantillons d'\cite{Anscombe1973}}
  \end{figure}

\end{frame}



\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}
  \framesubtitle {Propriétés statistiques de $\hat{\mathbf b}$}

  \begin{itemize}

  \item L'estimateur $\hat{\mathbf b}$ est une fonction de $X$ (supposé déterministe) et $\mathbf y$ (aléatoire).\newline

  \item[$\Rightarrow$] L'estimateur $\hat{\mathbf b}$ est une variable aléaoire.\newline

  \item[$\Rightarrow$] L'estimateur $\hat{\mathbf b}$ est toujours différent de $\beta$ (sauf si $T\rightarrow\infty$).\newline

  \end{itemize}


  \begin{prop}\label{prop:biais-variance}
    $\hat{\mathbf b}$ est un estimateur sans biais de $\beta$ (\underline{en moyenne} $\hat{\mathbf b}$ est égal à $\beta$), sa variance est~:
    \[
      \mathbb V \left[ \hat{\mathbf b} \right] = \sigma_{\varepsilon}^2(X'X)^{-1}
    \]
  \end{prop}

\end{frame}


\begin{notes}

  \textbf{Preuve de la proposition \ref{prop:biais-variance}}. Pour étudier les propriétés de l'estimateur des MCO, il faut substituer le processus générateur des données dans l'expression de l'estimateur. On a~:
  \[
    \begin{split}
      \hat{\mathbf b} &= (X'X)^{-1}X'\mathbf y\\
                      &= (X'X)^{-1}X'\left( X\bm \beta + \varepsilon \right)\\
                      &= \bm \beta + (X'X)^{-1}X'\varepsilon
    \end{split}
  \]
  Ainsi l'espérance de l'estimateur des MCO est~:
  \[
    \begin{split}
      \mathbb E \left[ \hat{\mathbf b} \right] &= \bm \beta + \mathbb E\left[ (X'X)^{-1}X'\varepsilon \right]\\
                                               &= \bm \beta + (X'X)^{-1}X'\mathbb E\left[ \varepsilon \right]\quad\text{car $X$ est déterministe}\\
                                               &= \bm \beta\quad\text{car $\varepsilon$  est d'espérance nulle.}
    \end{split}
  \]
  L'estimateur est donc sans biais, en moyenne l'estimateur des MCO est égal à la vraie valeur des paramètres. La variance est l'espérance du carré de l'écart à l'espérance~:
  \[
    \mathbb V\left[ \hat{\mathbf b} \right] = \mathbb E\left[\left( \hat{\mathbf b}- \bm\beta \right)\left( \hat{\mathbf b}- \bm\beta \right)'\right]
  \]
  une matrice $K\times K$. En substituant l'espression de l'estimateur en fonction de $\varepsilon$, on obtient~:
\[
    \begin{split}
      \mathbb V\left[ \hat{\mathbf b} \right] &= \mathbb E\left[ \left( (X'X)^{-1}X'\varepsilon \right)\left( (X'X)^{-1}X'\varepsilon \right)' \right]\\
                                              &= \mathbb E\left[ (X'X)^{-1}X'\varepsilon\varepsilon' X (X'X)^{-1} \right]\\
    \end{split}
  \]
Puisque $X$ est déterministe~:
  \[
    \begin{split}
      \mathbb V\left[ \hat{\mathbf b} \right]
                                              &= (X'X)^{-1}X' \mathbb E\left[ \varepsilon\varepsilon' \right]X(X'X)^{-1}\\
                                              &= (X'X)^{-1}X' \sigma_\varepsilon^2I_T X(X'X)^{-1}
    \end{split}
  \]
  par $\mathcal H_3$. Enfin~:
\[
    \begin{split}
      \mathbb V\left[ \hat{\mathbf b} \right]
                                              &= \sigma_\varepsilon^2(X'X)^{-1}X'X(X'X)^{-1}\\
      &= \sigma_\varepsilon^2(X'X)^{-1}
    \end{split}
  \]
  \qed


  \begin{itemize}
  \item L'estimateur des MCO est d'autant moins précis (sa variance est d'autant plus grande) que la variance des erreurs $\varepsilon$ est importante.\newline

  \item L'esimateur des MCO est d'autant moins précis que la variabilité des variables explicatives, la matrice $X'X$ \guillemotleft petite\guillemotright, est faible.\newline

  \item[$\Rightarrow$] Il est plus facile d'identifier l'effet d'une variable explicative sur $y$ si le signal (la variance de la variable explicative) est important relativement au bruit ($\sigma_{\varepsilon}^2$).
  \end{itemize}

\end{notes}


\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}
  \framesubtitle {Propriétés statistiques de $\hat{\mathbf b}$}

  \begin{figure}
    \centering
    \begin{subfigure}{0.4\textwidth}
      \scalebox{.3}{
    \input{images/chapitre-1/slope-estimate-sample-nonstochastic-x.tex}}
    \caption{$x$ déterministe.}
    \label{fig:01:a}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.4\textwidth}
    \scalebox{.3}{
    \input{images/chapitre-1/slope-estimate-sample-stochastic-x.tex}}
    \caption{$x$ stochastique.}
    \label{fig:01:b}
  \end{subfigure}
  \label{fig:01}
  \caption{Le modèle de la nature est $y_t = x_t + \varepsilon_t$ avec $x_t$ une variable exogène (déterministe ou stochastique) prenant des valeurs dans l'intervalle $[0,10]$ et $\varepsilon_t$ un bruit blanc gaussien. On simule 100000  échantillons de 10 observations, pour chaque échantillon on estime le modèle empirique $y_t = b_0 + b_1 x_t + \epsilon_t$. Chaque figure représente la distribution (avec un histogramme) de l'esimateur $\hat b_1$. Comme attendu,  $\hat b_1$ est centré sur 1, la vrai valeur de la pente.  Les codes pour reproduire ces graphiques sont disponibles \href{https://mnemosyne.ithaca.fr/stephane/econometrics/-/blob/\HEAD/cours/codes/chapitre-1/estimator-with-deterministic-versus-stochastic-samples.py}{ici}.}
\end{figure}
\end{frame}


\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}
  \framesubtitle {Propriétés statistiques de $\hat{\mathbf b}$}


  \begin{theorem}[Gauss Markov]\label{thm:Gauss-Markov}
    $\hat{\mathbf b}$ est le meilleur estimateur linéaire sans biais (BLUE) de $\beta$.
  \end{theorem}

  \bigskip\bigskip

  \textbf{Remarque.} L'estimateur $\lambda \hat{\mathbf b}$, où $\lambda$ est un vecteur de paramètres $1\times K$, est aussi le meilleur estimateur linéaire sans biais de $\lambda \beta$.
\end{frame}


\begin{notes}

  \textbf{Preuve de la proposition \ref{thm:Gauss-Markov}}. Nous avons déjà montré que $\hat{\mathbf b}$ est un estimateur sans biais de $\beta$. Soit un estimateur linéaire en $\mathbf y$~: $\tilde{\mathbf b} = C \mathbf y$ avec, sans perte de généralité, $C = (X'X)^{-1}X'' + D$. L'absence de biais pose des contraintes sur la matrice $D$~:
  \[
    \begin{split}
      \mathbb E\left[\tilde{\mathbf b}\right] &= \mathbb E\left[\left( (X'X)^{-1}X'+D \right)\left( X\beta + \varepsilon \right)\right]\\
      &= \beta + DX\beta
    \end{split}
  \]
  pour que l'estimateur $\tilde{\mathbf b}$ soit non biaisé, il faut et il suffit que la matrice $DX$ soit nulle. Calculons la variance de cet estimateur et montrons qu'elle est plus grande que la variance de l'estimateur des MCO, dans le sens où la matrice $\mathbb V\left[\tilde{\mathbf b}\right] - \mathbb V\left[\hat{\mathbf b}\right]$ est définie positive. Si $\tilde{\mathbf b}$ est sans biais, on a~:
  \[
    \begin{split}
      \mathbb V\left[\tilde{\mathbf b}\right] &= \mathbb E\left[\left( \tilde{\mathbf b}-\beta \right)\left( \tilde{\mathbf b}-\beta \right)'\right] \\
                                              &= \mathbb E\left[\left( \left( (X'X)^{-1}X'+D \right)\left( X\beta + \varepsilon \right) -\beta \right)\left( \left( (X'X)^{-1}X'+D \right)\left( X\beta + \varepsilon \right)-\beta \right)'\right]\\
                                              &= \mathbb E\left[ \left((X'X)^{-1}X'\varepsilon  + D\varepsilon + DX\beta \right)\left((X'X)^{-1}X'\varepsilon  + D\varepsilon + DX\beta \right)'\right]\\
                                              &= \mathbb E\left[ \left((X'X)^{-1}X'  + D\right)\varepsilon\varepsilon'\left((X'X)^{-1}X'  + D\right)'\right]\\
                                              &= \sigma_{\varepsilon}^2\left((X'X)^{-1}X'  + D\right)\left((X'X)^{-1}X'  + D\right)'\\
                                              &= \sigma_{\varepsilon}^2\left( (X'X)^{-1}  + DD'\right)\\
                                              &= \mathbb V\left[\hat{\mathbf b}\right] + \sigma_{\varepsilon}^2 DD'
    \end{split}
  \]
Comme $DD'$ est une matrice définie positive, la variance de $\tilde{\mathbf b}$ est plus grande que celle de $\hat{\mathbf b}$. L'estimateur des MCO est donc bien le meilleur, au sens de la réduction de la variance, estimateur linéaire sans biais de $\beta$.\qed
\end{notes}


\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}
  \framesubtitle {Propriétés statistiques de $\hat{\mathbf b}$}


  \begin{prop}\label{prop:mco:convergence}
    $\hat{\mathbf b}$ est un estimateur convergent de $\beta$~:
    \[
      \hat{\mathbf b} \underset{T\to\infty}{\overset{\text{proba}}{\longrightarrow}} \beta
    \]
  \end{prop}

  \bigskip\bigskip

  \begin{itemize}

  \item Ce résultat nous dit que la probabilité d'observer une différence arbitrairement petite entre l' estimateur des MCO et $\beta$ tend vers zéro quand la taille de l'échantillon tend vers l'infini.\newline

  \item La distribution de l'estimateur $\hat{\mathbf b}$ se concentre autour de $\beta$ quand la taille de l'échantillon tend vers l'infini.

  \end{itemize}

\end{frame}

\clearpage

\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}
  \framesubtitle {Propriétés statistiques de $\hat{\mathbf b}$}

  \begin{figure}
    \centering
      \scalebox{.425}{
    \input{images/chapitre-1/ols-convergence.tex}}
  \caption{Le modèle de la nature est $y_t = x_t + \varepsilon_t$ avec $x_t$ une variable exogène déterministe prenant des valeurs dans l'intervalle $[0,10]$ et $\varepsilon_t$ un bruit blanc gaussien. On simule 100000  échantillons de 10, 100 ou 1000 observations, pour chaque échantillon on estime le modèle empirique $y_t = b_0 + b_1 x_t + \epsilon_t$. Chaque figure représente la distribution de l'esimateur $\hat b_1$. Les codes pour reproduire ces graphiques sont disponibles \href{https://mnemosyne.ithaca.fr/stephane/econometrics/-/blob/\HEAD/cours/codes/chapitre-1/ols-convergence.py}{ici}.}
\end{figure}
\end{frame}


\begin{notes}

  \textbf{Preuve de la proposition \ref{prop:mco:convergence}.} Nous savons déjà que l'estimateur est sans biais, pour toute dimension de l'échantillon. Pour établir la convergence en probabilité de l'estimateur vers $\beta$, il suffit de montrer que sa variance tend vers 0 quand $T$ tend vers l'infini. Par $\mathcal H_2$, nous savons que~:
\[
      \lim_{T\rightarrow\infty} \frac{X'X}{T} = Q
    \]
    où $Q$ est une matrice définie positive. Ainsi, nous avons~:
    \[
      \begin{split}
        \lim_{T\to\infty} (X'X)^{-1} &= \lim_{T\to\infty} \frac{1}{T}\left(\frac{X'X}{T}\right)^{-1}\\
                                     &= Q^{-1}\lim_{T\to\infty} \frac{1}{T}\\
                                     &= 0
      \end{split}
    \]
    et donc, par définition la variance de l'estimateur~: $\lim_{T\to\infty} \mathbb V[\hat{\mathbf b}] = 0$.\qed
\end{notes}


\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}
  \framesubtitle {Retour sur les résidus estimés}

  \begin{itemize}

  \item On peut exprimer $\hat\epsilon$ comme une fonction linéaire de $\varepsilon$~:
    \[
      \begin{split}
        \hat\epsilon &= \mathbf y-X\hat{\mathbf b}\\
                     &= \mathbf y - X(X'X)^{-1}X'\mathbf y\\
                     &= X\mathbf\beta + \varepsilon - X(X'X)^{-1}X'\left( X\mathbf\beta + \varepsilon\right)\\
                     &= \left( I -  X(X'X)^{-1}X'\right)\varepsilon
      \end{split}
    \]

    \medskip

  \item On notera~: $P = X(X'X)^{-1}X'$, la matrice de projection orthogonale, et $M = I-P$. On a~:
    \[
      \hat\epsilon = M\varepsilon
    \]

    \medskip

  \item La matrice $M$ est symétrique et idempotente~:
    \[
      M' = I - \left( X(X'X)^{-1}X' \right) = I - X(X'X)^{-1}X = M
    \]
    \[
      M'M = I-2X(X'X)^{-1}X'+\underbrace{X(X'X)^{-1}X'X(X'X)^{-1}X'}_{X(X'X)^{-1}X'}= M
    \]
  \end{itemize}

\end{frame}


\begin{notes}

  \begin{itemize}

  \item Les valeurs propres d'une matrice idempotente $A$ sont égales à 1 ou 0. En effet, supposons que $\lambda$ soit une valeur propre et $x$ le vecteur propre associé. Alors, par définition d'une valeur propre, on doit avoir~:
    \[
      \lambda x = A x
    \]
    Comme $A$ est idempotente, on a $\lambda x = A^2 x$, ou encore~:
    \[
      \begin{split}
        \lambda x &= A \lambda x\\
        &= \lambda A x
      \end{split}
    \]
    et donc~:
    \[
      \lambda x = \lambda^2 x
    \]
    ce qui n'est possible que si $\lambda$ est égal à un ou zéro.\newline

  \item Ainsi le rang d'une matrice idempotente est égal à sa trace (la somme des éléments sur la diagonale).\newline

  \item Nous avons donc~:
    \[
      \begin{split}
        \mathrm{rang}(M) &= \mathrm{trace}(I_T) - \mathrm{trace}\left( X(X'X)^{-1}X' \right)\\
                         &= T - \mathrm{trace}\left( X'X(X'X)^{-1} \right)\\
                         &= T - \mathrm{trace}\left(I_K\right)\\
        &= T-K
      \end{split}
  \]
  \end{itemize}

\end{notes}


\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}
  \framesubtitle {Retour sur les résidus estimés}

  \begin{itemize}

  \item L'expression des résidus estimés en fonction de $\varepsilon$ permet de déduire des propriétés probabilistes sur $\hat\epsilon$.\newline

  \item On montre facilement que $\mathbb E[\hat\epsilon] = 0$ et $\mathbb V[\hat\epsilon] = \sigma_{\varepsilon}^2M$.\newline

  \item Même si la matrice de variance covariance de $\varepsilon$ est diagonale, les résidus estimés sont corrélés (car la matrice $M$ n'est pas diagonale).\newline

  \item La normalité de $\varepsilon$ implique la normalité de $\hat\epsilon$, \underline{\textbf{mais}} la distribution de $\hat\epsilon$ est dégénére (sa variance n'est pas de plein rang)... Nous ne sommes pas surpris puisque nous avons déjà montré que la somme des résidus estimés doit être nulle (déterministe).\newline

  \item On peut aussi exprimer la somme des carrés des résidus estimés comme une fonction de $\varepsilon$~:
    \[
      SSE = \hat\epsilon'\hat\epsilon = \varepsilon'M'M \varepsilon = \varepsilon' M \varepsilon
    \]

  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}
  \framesubtitle {Comment estimer $\sigma_{\varepsilon}^2$~?}

  \begin{prop}\label{prop:sse:expectation}
    L'espérance de somme des carrés des résidus estimés est donnée par~:
    \[
      \mathbb E [SSE] = \sigma_{\varepsilon}^2(T-K)
    \]
  \end{prop}


  \begin{cor}\label{cor:s2}
    Un estimateur non biaisé de $\sigma_{\varepsilon}^2$ est~:
    \[
      s^2 = \frac{SSE}{T-K}
    \]
  \end{cor}


  \begin{prop}\label{prop:s2}
    $s^2$ est un estimateur convergent de $\sigma_{\varepsilon}^2$.
  \end{prop}

\end{frame}


\begin{notes}
  \begin{itemize}

  \item \textbf{Preuve de la proposition \ref{prop:sse:expectation}.} On a~:
    \[
      \begin{split}
        \mathbb E[SSE] &= \mathbb E [\varepsilon' M \varepsilon]\\
                       &= \mathbb E \left[\mathrm{trace}\left( \varepsilon' M \varepsilon \right)\right]\\
                       &= \mathbb E \left[\mathrm{trace}\left( M \varepsilon\varepsilon'  \right)\right]\\
                       &= \mathrm{trace}\left( M \sigma_{\varepsilon}^2 I_T  \right)\\
                       &= \sigma_{\varepsilon}^2\mathrm{trace}\left( M\right)\\
                       &= \sigma_{\varepsilon}^2(T-K)
      \end{split}
    \]
\qed

\medskip

En passant, notons que $\frac{SSE}{T}$, qui nous le verrons plus loin est l'estimateur du maximum de vraisemblance, est un estimateur biaisé à distance finie. Asymptotiquement, il n'y a cependant pas de différence entre l'estimateur du maximum de vraisemblance est $s^2$.\newline

\medskip
\item \textbf{Lemme.} On a~: $\underset{T\to\infty}{\mathrm{plim}} \frac{\varepsilon'X}{T}$.\newline\newline

  \textbf{Preuve.} Puisque $X$ est déterministe, on a directement:~ $\mathbb E \left[ \frac{\varepsilon'X}{T} \right] = 0$. Tout assi facilement on montre que la variance est~:
  \[
    \mathbb V \left[ \frac{\varepsilon'X}{T} \right] = \frac{\sigma_{\varepsilon}^2}{T}\frac{X'X}{T}\underset{T\to\infty}{\longrightarrow} 0 \times Q=0
  \]
$\frac{\varepsilon 'X}{T}$ converge donc bien en probabilité vers 0.\qed

\pagebreak

\item \textbf{Preuve de la proposition \ref{prop:s2}.} Nous devons montrer que $s^2$ converge en probabilité vers $\sigma_{\varepsilon}^2$. On a~:
  \[
    \begin{split}
      \underset{T\to\infty}{\mathrm{plim}} s^2 &= \underset{T\to\infty}{\mathrm{plim}} \frac{\varepsilon' M \varepsilon}{T-K} = \underset{T\to\infty}{\mathrm{plim}} \frac{\varepsilon' M \varepsilon}{T}\\
                                               &= \underset{T\to\infty}{\mathrm{plim}} \frac{\varepsilon' \varepsilon}{T} - \underset{T\to\infty}{\mathrm{plim}} \frac{\varepsilon' X (X'X)^{-1}X'\varepsilon}{T}\\
                                               &= \sigma_{\varepsilon}^2 - \underset{T\to\infty}{\mathrm{plim}} \frac{\varepsilon'X}{T}\left( \frac{X'X}{T} \right)^{-1}\frac{X'\varepsilon}{T}\\
      & \sigma_{\varepsilon}^2 - 0 \times Q^{-1} \times 0 = \sigma_{\varepsilon}^2
    \end{split}
  \]
\qed
\end{itemize}
\end{notes}


\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}
  \framesubtitle {Distribution de $\hat{\mathbf b}$ et $s^2$}

  \begin{theorem}\label{thm:bhat:distribution}
    L'estimateur $\hat{\mathrm b}$ est normalement distribué~:
    \[
      \hat{\mathbf b} \sim \mathcal N\left( \beta, \sigma_{\varepsilon}^2(X'X)^{-1} \right)
    \]
  \end{theorem}


  \begin{theorem}\label{thm:s2:distribution}
    L'estimateur $\frac{(T-K)s^2}{\sigma_{\varepsilon}^2}$ est distribué comme un khi-2~:
    \[
      \frac{(T-K)s^2}{\sigma_{\varepsilon}^2} \sim \chi^2(T-K)
    \]
  \end{theorem}


  \begin{cor}\label{cor:s2:distribution}
    La variance de $s^2$ est égale à $\frac{2\sigma_{\varepsilon}^4}{T-K}$.
  \end{cor}

\end{frame}


\begin{notes}

  \begin{itemize}

  \item \textbf{Preuve du théorème \ref{thm:bhat:distribution}.} Nous avons déjà obtenu l'espérance et la variance de $\hat{\mathrm b}$. Comme l'estimateur des MCO est linéaire par rapport à $\mathbf y$ et donc $\varepsilon$ (puisque le modèle de la nature est linéaire) qui suit une loi normale multivariée. L'estimateur des MCO est donc normalement distribué.\qed\newline

  \item \textbf{Lemme.} Soit $Q$ une matrice réelle $T\times T$ symétrique et idempotente, alors~:
       \[
         \frac{\varepsilon' Q \varepsilon}{\sigma_{\varepsilon}^2} \sim \chi^2\left(\mathrm{trace}(Q)\right)
       \]
       \medskip

       \textbf{Preuve.} Puisque la matrice réelle $Q$ est symétrique,
       on sait qu'il existe une matrice orthogonale $P$ telle
       que $P'QP = \Lambda$ est une matrice diagonale. On sait que les
       valeurs propres de $Q$ sont égales à 1 ou 0. Sans perte de
       généralité, on suppose que les valeurs propres non nulles
       viennent en premier le long de la diagonale de $\Lambda$~:
       \[
         \Lambda =
         \begin{pmatrix}
           I_{\star} & 0 \\
           0 & 0
         \end{pmatrix}
       \]
       où la dimension de $I_{\star}$ est égale à la trace de la
       matrice $Q$ (la trace d'une matrice est invariante au
       changement de base). Définissons $\nu = P'\varepsilon$, on a
       directement $\mathbb E[\nu] = 0$
       et $\mathbb V[\nu] = \sigma_{\varepsilon}^2I_T$ puisque $P$ est
       une matrice orthogonale. Le vecteur $\nu$ est normalement
       distribué~:
       $\nu \sim \mathcal N \left( 0, \sigma_{\varepsilon}^2I_T \right)$. Puisque
       $P$ est une matrice orthogonale, nous pouvons
       exprimer $\varepsilon$ en fonction de $\nu$~:
       \[
         P\nu = PP'\varepsilon \Leftrightarrow \varepsilon = P\nu
       \]
       On a donc~:
       \[
         \begin{split}
           \frac{\varepsilon' Q \varepsilon}{\sigma_{\varepsilon}^2} &= \frac{\nu'P'Q P \nu}{\sigma_{\varepsilon}^2}\\
                                                                     &= \frac{1}{\sigma_{\varepsilon}^2}\nu'
                                                                       \begin{pmatrix}
                                                                         I_{\star} & 0\\
                                                                         0 & 0
                                                                       \end{pmatrix} \nu \\
                                                                     &= \frac{1}{\sigma_{\varepsilon}^2}\sum_{i=1}^{\mathrm{tr}(Q)}\nu_i^2\\
                                                                     &= \sum_{i=1}^{\mathrm{tr}(Q)}\left( \frac{\nu_i}{\sigma_{\varepsilon}} \right)^2
         \end{split}
       \]
       Puisque $\frac{\nu_i}{\sigma_\varepsilon} \perp \frac{\nu_j}{\sigma_\varepsilon} \sim \mathcal N(0,1)$ pour tout $i\neq j$, on a $\frac{\varepsilon' Q \varepsilon}{\sigma_{\varepsilon}^2}\sim\chi^2\left( \mathrm{tr}(Q) \right)$.\newline

     \item \textbf{Preuve du théorème \ref{thm:s2:distribution}.} Nous avons~:
       \[
         \frac{(T-K)s^2}{\sigma_{\varepsilon}^2} = \frac{SSE}{\sigma_{\varepsilon}^2} = \frac{\varepsilon' M \varepsilon}{\sigma_{\varepsilon}^2}
       \]
       où la matrice $M = I - X(X'X)^{-1}X'$ est une matrice de rang $T-K$. Le lemme précédent entraîne le résultat annoncé.\qed\newline

     \item \textbf{Preuve du corollaire \ref{cor:s2:distribution}.} Direct en notant que la variance d'un $\chi^2$ à $T-K$ degrés de liberté est $2(T-K).$\qed

     \end{itemize}

\end{notes}


\begin{frame}
  \frametitle{Estimateur des Moindres Carrés Ordinaires}
  \framesubtitle {Distribution de $\hat{\mathbf b}$ et $s^2$}

  \begin{prop}\label{prop:bhat_perp_s2}
    Les estimateurs $\hat{\mathrm b}$ et $s^2$ sont des variables aléatoires indépendantes.
  \end{prop}

\end{frame}


\begin{notes}

  \begin{itemize}

  \item \textbf{Lemme}
    Soit $Q$ une matrice réelle $T\times T$ symétrique et idempotente de rang $q$. Soit $B$ une matrice réelle $m\times T$ telle que $BQ=0$. Soit le vecteur aléatoire gaussien $\varepsilon\sim\mathcal N(0, \sigma_{\varepsilon}^2I_T)$. Alors le vecteur aléatoire $B\varepsilon$ et la variable aléatoire $\varepsilon' Q \varepsilon$ sont indépendants.\newline

    \textbf{Preuve.} Comme dans la preuve du lemme précédent, on définit la matrice orthogonale $P$ telle que~:
    \[
      P'QP =
      \begin{pmatrix}
        I_q & 0\\
        0   & 0
      \end{pmatrix}
    \]
    et le vecteur gaussien $\nu = P'\varepsilon\sim\mathcal N\left(0, \sigma_{\varepsilon}^2I_T\right)$ (car $P$ est une matrice orthogonale). On partitionne le vecteur $\nu$ sous la forme $(\nu_1',\nu_2')'$ où $\nu_1$ et $\nu_2$ sont des vecteurs aléatoires $q\times 1$ et $(T-q)\times 1$ indépendants. On a directement~:
    \[
      \varepsilon' Q \varepsilon = \nu_1'\nu_1
    \]
    qui suit un $\chi^2(q)$. Posons $BP = C$ et partitionnons les colonnes de $C$ conformément à la partition de $\nu$:
    \[
      C =
      \begin{pmatrix}
        C_1 & C_2
      \end{pmatrix}
    \]
    où $C_1$ est une matrice $m\times q$ et $C_2$ une matrice $m\times(T-q)$. Nous devons avoir $C P' Q P = 0$ puisque $CP'QP = BPP'QP = BQP$ et $BQ = 0$ par hypothèse. Ainsi~:
    \[
      \begin{pmatrix}
        C_1 & C_2
      \end{pmatrix}
      \begin{pmatrix}
        I_q & 0\\
        0   & 0
      \end{pmatrix}
      = 0
    \]
    et donc $C_1=0$. Pour que la matrice $BQ$ soit nulle, il faut éliminer toutes les valeurs propres non nulles. En notant que~:
    \[
      B\varepsilon = BPP'\varepsilon = C\nu = C_2\nu_2
    \]
    puisque $C_1$ doit être nul, on voit que $B\varepsilon$ ne dépend que de $\nu_2$ qui est indépendant de $\nu_1$ donc de $\varepsilon' Q \varepsilon$.\qed\newline

  \item \textbf{Preuve de la proposition \ref{prop:bhat_perp_s2}.} On
    a $s^2=\frac{\varepsilon' M \varepsilon}{T-K}$
    et
    $\hat{\mathbf b}-\beta = (X'X)^{-1}X'\varepsilon$. Puisque
    $(X'X)^{-1}X'M = (X'X)^{-1}X'-(X'X)^{-1}X'X(X'X)^{-1}X'=0$, le
    lemme implique que $s^2$ et $\hat{\mathbf b}$ sont indépendants.\qed


  \end{itemize}

\end{notes}


\begin{frame}
  \frametitle{Vraisemblance}

  \begin{itemize}

  \item La vraisemblance est la densité de l'échantillon.\newline

  \item Comme le modèle est linéaire et que $\varepsilon$ est normalement distribué, on a directement~:
    \[
      L(\beta, \sigma_{\varepsilon}^2; \mathbf y) = (2\pi\sigma_{\varepsilon}^2)^{-\frac{T}{2}}e^{-\frac{1}{2\sigma_{\varepsilon}^2}(\mathbf y - X\beta)'(\mathbf y - X\beta)}
    \]
    \medskip

  \item En pratique, par la suite, on considérera souvent la log-vraisemblance notée $l(\beta, \sigma_{\varepsilon}^2; \mathbf y)$~:
    \[
      l(\beta, \sigma_{\varepsilon}^2; \mathbf y) = -\frac{T}{2}\log(2\pi) -\frac{T}{2}\log\sigma_{\varepsilon}^2 - \frac{1}{2\sigma_{\varepsilon}^2}(\mathbf y - X\beta)'(\mathbf y - X\beta)
    \]
    \medskip

  \item $\underset{\{\beta, \sigma_{\varepsilon}^2\}}{\max} l(\beta, \sigma_{\varepsilon}^2; \mathbf y) \Leftrightarrow \underset{\{\beta, \sigma_{\varepsilon}^2\}}{\max} L(\beta, \sigma_{\varepsilon}^2; \mathbf y)$

  \end{itemize}

\end{frame}


\begin{notes}

  \begin{itemize}

  \item On peut écrire la vraisemnblance de façon équivalente sous la forme~:
    \[
      L(\beta, \sigma_{\varepsilon}^2; \mathbf y) = (2\pi\sigma_{\varepsilon}^2)^{-\frac{T}{2}}e^{-\frac{SSE + \left(\hat{\mathbf b}-\beta\right)'X'X\left(\hat{\mathbf b}-\beta\right)}{2\sigma_{\varepsilon}^2}}
    \]

    \medskip

    Développons le numérateur sous l'exponentielle~:

    \begin{align*}
      (\mathbf y - X\beta)'(\mathbf y - X\beta) &= \left(\mathbf y - X\hat{\mathbf b} + X(\hat{\mathbf b}-\beta)\right)'\left(\mathbf y - X\hat{\mathbf b} + X(\hat{\mathbf b}-\beta)\right)\\
                                                &= (\mathbf y - X\hat{\mathbf b})'(\mathbf y - X\hat{\mathbf b})
                                                     + (\hat{\mathbf b}-\beta)'X'X(\hat{\mathbf b}-\beta)\\
                                                & \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad   + 2(\hat{\mathbf b}-\beta)X'(\mathbf y-X\hat{\mathrm b})\\
                                                   &= SSE + (\hat{\mathbf b}-\beta)'X'X(\hat{\mathbf b}-\beta)
      \end{align*}

    où sur la deuxième ligne le dernier terme est nul car les résidus estimés sont orthogonaux aux variables explicatives.\newline


  \item $SSE$ et $\hat{\mathbf b}$ sont des statistiques suffisantes, du point de vue de la vraisemblance et donc de l'inférence, elles résument parfaitement les données.\newline

  \item Comme l'estimateur des MCO pour $\beta$
    et $\sigma_{\varepsilon}^2$ ne dépendent que des statistiques
    suffisantes et que ces estimateurs sont sans biais, on peut
    montrer que ces estimateurs sont efficaces ($\nexists$
    d'estimateur sans biais avec une variance plus faible).

  \end{itemize}

\end{notes}


\begin{frame}
  \frametitle{Vraisemblance}

  \begin{itemize}

  \item Les dérivées partielles de la log-vraisemblance~:
    \[
      \frac{\partial l(\beta, \sigma_{\varepsilon}^2; \mathbf y)}{\partial\beta} = -\frac{1}{\sigma_{\varepsilon}^2}X'(\mathbf y - X\beta)
    \]
    \[
      \frac{\partial l(\beta, \sigma_{\varepsilon}^2; \mathbf y)}{\partial\sigma_{\varepsilon}^2} = -\frac{T}{2\sigma_{\varepsilon}^2} + \frac{1}{2\sigma_{\varepsilon}^4}\left( \mathbf y - X\beta \right)'\left( \mathbf y - X\beta \right)
    \]
    \medskip

  \item Les espérances des dérivées partielles sont nulles~:

    \[
      \mathbb E\left[ \frac{\partial l(\beta, \sigma_{\varepsilon}^2; \mathbf y)}{\partial\beta} \right] = -\frac{1}{\sigma_{\varepsilon}^2}X'\mathbb E[\varepsilon] = 0
    \]
    \[
      \mathbb E \left[ \frac{\partial l(\beta, \sigma_{\varepsilon}^2; \mathbf y)}{\partial\sigma_{\varepsilon}^2} \right] = -\frac{T}{2\sigma_{\varepsilon}^2} + \frac{1}{2\sigma_{\varepsilon}^4}\mathbb E[\varepsilon'\varepsilon] = -\frac{T}{2\sigma_{\varepsilon}^2} + \frac{T\sigma_{\varepsilon}^2}{2\sigma_{\varepsilon}^4} = 0
    \]

  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Vraisemblance}

  \begin{prop}\label{prop:cdfr}
    Les bornes inférieures de Cramer-Darmois-Fréchet-Rao pour les variances des estimateurs sans biais de $\beta$ et $\sigma_{\varepsilon}^2$ sont $\sigma_{\varepsilon}^2(X'X)^{-1}$ et $\frac{2\sigma_{\varepsilon}^4}{T}$.
  \end{prop}

  \bigskip

  \begin{itemize}

  \item La variance de $\hat{\mathbf b}$ est sur la borne CDFR, il s'agit donc bien d'un estimateur efficace (sans biais et de variance minimale).\newline

  \item La variance de $s^2$ est strictement supérieure à la borne CDFR. Il s'agit tout de même d'un estimateur efficace car il ne dépend que d'une statistique suffisante ($SSR$, par le théorème de Lehmann-Scheffé).

  \end{itemize}

\end{frame}


\begin{notes}

  \begin{itemize}

  \item \textbf{Preuve de la proposition \ref{prop:cdfr}.} Calculons la matrice d'information de Fisher. Les dérivées secondes de la log-vraisemblance sont~:
    \[
      \frac{\partial^2 l(\beta, \sigma_{\varepsilon}^2; \mathbf y)}{\partial\beta\partial\beta'} = -\frac{1}{\sigma_{\varepsilon}^2}X'X
    \]
    \[
      \frac{\partial^2 l(\beta, \sigma_{\varepsilon}^2; \mathbf y)}{\partial\sigma_{\varepsilon}^4} = \frac{T}{2\sigma_{\varepsilon}^4}-\frac{1}{\sigma_{\varepsilon}^6}\left( \mathbf y-X\beta \right)'\left( \mathbf y-X\beta \right)
    \]
    \[
      \frac{\partial^2 l(\beta, \sigma_{\varepsilon}^2; \mathbf y)}{\partial\beta\partial\sigma_{\varepsilon}^2} = -\frac{1}{\sigma_{\varepsilon}^4}X'\left( \mathbf y - X\beta \right)
    \]
    La matrice d'information de Fisher est l'opposé de l'espérance de la matrice hessienne (des dérivées secondes)~:
    \[
      I =
      \begin{pmatrix}
        \frac{X'X}{\sigma_{\varepsilon}^2} & 0 \\
        0                                  & \frac{T}{2\sigma_{\varepsilon}^4}
      \end{pmatrix}
    \]
    La borne CDFR est définie par la diagonale de l'inverse de la matrice d'information de Fisher.\qed

  \end{itemize}

\end{notes}


\begin{frame}
  \frametitle{Vraisemblance}

  \begin{prop}\label{prop:ml}
    Les estimateurs du maximum de vraisemblance de $\beta$ et $\sigma_{\varepsilon}^2$ sont~:
    \[
      \hat\beta = (X'X)^{-1}X'\mathbf y \quad\text{ et }\quad \hat{\sigma}_{\varepsilon}^2 = \frac{SSE}{T}
    \]
  \end{prop}

  \begin{itemize}

  \item L'estimateur du MV de $\beta$ est identique à l'estimateur des MCO $\hat{\mathbf b}$.\newline

  \item L'estimateur du MV de $\sigma_{\varepsilon}^{2}$ est biaisé !\newline

  \item La variance de l'estimateur du MV de $\sigma_{\varepsilon}^{2}$ est plus faible que la variance de $s^2$~:
    \[
      \mathbb V\left[ \hat{\sigma}_{\varepsilon}^2 \right] = 2\sigma_{\varepsilon}^4\frac{T-K}{T^2} < \underbrace{\frac{2\sigma_{\varepsilon}^4}{T}}_{\textsc{CDFR}}< \frac{2\sigma_{\varepsilon}^4}{T-K}
    \]
  \item \underline{\textbf{Mais}} l'erreur quadratique moyenne de l'estimateur du MV est plus faible que celle de $s^2$.

  \end{itemize}

\end{frame}


\begin{notes}

  \begin{itemize}

  \item \textbf{Preuve de la proposition \ref{prop:ml}} Direct en annulant les dérivées partielles par rapport à $\beta$ et $\sigma_{\varepsilon}^2$. Pour $\hat\beta$ on peut alternativement raisonner sur la représentation alternative (en fonction de $SSE$ et $\hat{\mathbf b}$) de la fonction de vraisemblance.\qed\newline

  \item \textbf{Calcul de la variance de $\hat{\sigma}_{\varepsilon}^2$.} Nous avons déjà montré, voir le corollaire \ref{cor:s2:distribution}, que~:
    \[
      \mathbb V\left[ \frac{SSE}{T-K} \right] = \frac{2\sigma_{\varepsilon}^4}{T-K}
    \]
    \[
      \Leftrightarrow \mathbb V\left[ \frac{SSE}{T}\frac{T}{T-K} \right] = \frac{2\sigma_{\varepsilon}^4}{T-K}
    \]
    \[
      \Leftrightarrow \mathbb V\left[ \frac{SSE}{T}\right] = \frac{2\sigma_{\varepsilon}^4}{T-K}\frac{(T-K)^2}{T^2}
    \]
    \[
      \Leftrightarrow \mathbb V\left[ \hat{\sigma}_{\varepsilon}^2\right] = 2\sigma_{\varepsilon}^4\frac{T-K}{T^2}
    \]
    \qed\newline

  \item \textbf{Remarque 1.} La variance de l'estimateur du maximum de vraisemblance de $\sigma_{\varepsilon}^2$ est strictement inférieure à la borne de CDFR. Possible car cette borne inférieure ne concerne que les estimateurs sans biais (l'estimateur du MV est biaisé).\newline

  \item L'erreur quadratique moyenne d'un estimateur $\hat\theta$ est défini par~:
    \[
      MSE(\hat\theta) = \mathbb E\left[ \left( \hat\theta -\theta \right)^2 \right]
    \]
  \item Si $\hat\theta$ est un estimateur sans biais de $\theta$ alors l'erreur quadratique moyenne est égale à la variance.\newline

  \item Autrement, en notant $B(\hat\theta) = \mathbb E\left[\hat\theta\right]-\theta$ le biais de l'estimateur, on a~:
    \[
      \begin{split}
        MSE\left( \hat\theta \right) &= \mathbb E \left[ \left( \hat\theta - \mathbb E\left[\hat\theta\right] + B\left( \hat\theta \right) \right)^2 \right]\\
                                     &= \mathbb V\left[ \hat\theta \right] + B\left( \hat\theta \right)^2 + 2B\left(\hat\theta\right)\mathbb E\left[ \hat\theta - \mathbb \mathbb E\left[ \hat\theta \right] \right]\\
        &= \mathbb V\left[ \hat\theta \right] + B\left( \hat\theta \right)^2
      \end{split}
    \]

  \item \textsc{Arbitrage Biais-Variance.} Au sens de la réduction de l'erreur quadratique moyenne, un estimateur peut compenser son biais par une plus faible variance (une plus grande précision)\newline

  \item Dans le cas qui nous intéresse, on a~:
    \[
      MSE(s^2) = \mathbb V\left[ s^2 \right] = \sigma_{\varepsilon}^4\frac{2}{T-K}
    \]
    et
    \[
      MSE(\hat{\sigma}_{\varepsilon}^2) = \sigma_{\varepsilon}^4 \frac{2(T-K)+K^2}{T^2}
    \]
    Définissons~:
    \[
      r(s^2, \hat{\sigma}_{\varepsilon}^2) = \frac{MSE(s^2)}{MSE(\hat{\sigma}_{\varepsilon}^2)}
    \]
    \[
      \Leftrightarrow r(s^2, \hat{\sigma}_{\varepsilon}^2) = \frac{2}{T-K}\frac{T^2}{2(T-K)+K^2}
    \]
    On a~:
    \[
      \begin{split}
        r(s^2, \hat{\sigma}_{\varepsilon}^2) >1 &\Leftrightarrow 2T^2 > (T-K)\left[ 2(T-K) + K^2 \right]\\
                                                &\Leftrightarrow 2T^2 > 2T^2 + 2K^2 -4TK + K^2\\
                                                &\Leftrightarrow K(3K-4T)<0\\
                                                &\Leftrightarrow T>\frac{3}{4}K
      \end{split}
    \]
    La dernière inégalité est nécessairement vraie, autrement $X'X$ ne serait pas de plein rang (le nombre d'observations doit être supérieur aux nombre de variables explicatives). Le ratio $r(s^2, \hat{\sigma}_{\varepsilon}^2)$ doit donc être supérieur à 1, et l'erreur quadratique moyenne de l'estimateur du MV inférieure à l'erreur quadratique moyenne de $s^2$.\newline

  \item \textbf{Remarque 2.} Même si en moyenne $\hat{\sigma}_{\varepsilon}^2$ est différent de $\sigma_{\varepsilon}^2$, cet estimateur est en moyenne moins éloigné de  $\sigma_{\varepsilon}^2$ que $s^2$ (qui pourtant est égal à  $\sigma_{\varepsilon}^2$ en moyenne).

  \end{itemize}

\end{notes}


\begin{frame}
  \frametitle{Distribution asymptotique des estimateurs}

  \begin{prop}\label{prop:asymptotic_dist:bhat}
    L'estimateur $\hat{\mathbf b}$ est asymptotiquement normalement distribué~:
    \[
      \sqrt{T}\left( \hat{\mathbf b} - \beta\right) \underset{T\to\infty}{\Longrightarrow} \mathcal N\left( 0, \sigma_{\varepsilon}^2 Q^{-1}\right)
    \]
    où $Q = \lim_{T\to\infty}\frac{X'X}{T}$.
  \end{prop}

  \bigskip

  \begin{prop}\label{prop:asymptotic_dist:s2}
    L'estimateur $s^2$ est asymptotiquement normalement distribué~:
    \[
      \sqrt{T}\left( s^2 - \sigma_{\varepsilon}^2\right) \underset{T\to\infty}{\Longrightarrow} \mathcal N\left( 0, 2\sigma_{\varepsilon}^4\right)
    \]
  \end{prop}

\end{frame}


\begin{notes}

  \begin{itemize}

  \item \textbf{Preuve de la proposition \ref{prop:asymptotic_dist:bhat}.} Nous avons déjà montré, voir le théorème \ref{thm:s2:distribution}, que $\hat{\mathbf b}$ est normalement distribué pour tout $T$. Nous avons~:
    \[
      \hat{\mathbf b} \sim \mathcal N \left( \beta, \sigma_{\varepsilon}^2(X'X)^{-1}\right)
    \]
    \[
      \Leftrightarrow \hat{\mathbf b}-\beta \sim \mathcal N \left( 0, \sigma_{\varepsilon}^2(X'X)^{-1}\right)
    \]
    \[
      \Leftrightarrow \sqrt{T}\left( \hat{\mathbf b}-\beta \right) \sim \mathcal N \left( 0, T \sigma_{\varepsilon}^2(X'X)^{-1}\right)
    \]
    \[
      \Leftrightarrow \sqrt{T}\left( \hat{\mathbf b}-\beta \right) \sim \mathcal N \left( 0, \sigma_{\varepsilon}^2\left( \frac{X'X}{T} \right)^{-1}\right)
    \]
    On obtient le résultat asymptotique en rapellant que, selon $\mathcal H_2$, $\frac{X'X}{T}$ converge vers $Q$ lorsque $T$ tend vers l'infini.\qed\newline

  \item \textbf{Preuve de la proposition \ref{prop:asymptotic_dist:s2}.} Une variable aléatoire du $\chi^2(n)$ est une somme de $n$ variables aléatoires indépendantes (carrés de variables aléatoires normales centrées-réduites). Le théorème de la limite centrale nous dit que la variable du $\chi^2$ doit être vers une variable aléatoire normale quand $n$ tend vers l'infini.\newline

    D'après le théorème \ref{thm:s2:distribution}, on a~:
    \[
      \frac{(T-K)s^2}{\sigma_{\varepsilon}^2} = \sum_{i=1}^{T-K}v_i^2
    \]
    où $(v_i, i=1,\dots,T-K)$ sont des variables aléatoires gaussiennes centrées réduites indépendantes, et donc $(v_i^2, i=1,\dots,T-K)$ sont des $\chi^2(1)$ indépendantes. Sachant que $\mathbb E\left[ v_i^2 \right]=1$ et $\mathbb V\left[ v_i^2 \right]=2$, le théorème de la limite centrale nous dit que~:
    \[
      \frac{1}{\sqrt{T-K}}\sum_{i=1}^{T-K}\frac{v_i^2-1}{\sqrt{2}} \underset{T\to\infty}{\Longrightarrow} \mathcal N(0,1)
    \]
    \[
      \Leftrightarrow \frac{1}{\sqrt{T-K}}\sum_{i=1}^{T-K} \left( v_i^2-1 \right) \underset{T\to\infty}{\Longrightarrow} \mathcal N(0,2)
    \]
    \[
      \Leftrightarrow \frac{1}{\sqrt{T-K}}\left( \sum_{i=1}^{T-K}v_i^2 - (T-K) \right) \underset{T\to\infty}{\Longrightarrow} \mathcal N(0,2)
    \]
    \[
      \Leftrightarrow \frac{1}{\sqrt{T-K}}\left( \frac{(T-K)s^2}{\sigma_{\varepsilon}^2} - (T-K) \right) \underset{T\to\infty}{\Longrightarrow} \mathcal N(0,2)
    \]
    \[
      \Leftrightarrow \sqrt{T-K}\left( s^2 - \sigma_{\varepsilon}^2 \right) \underset{T\to\infty}{\Longrightarrow} \mathcal N\left(0,2\sigma_{\varepsilon}^4 \right)
    \]
    \[
      \Rightarrow \sqrt{T}\left( s^2 - \sigma_{\varepsilon}^2 \right) \underset{T\to\infty}{\Longrightarrow} \mathcal N\left(0,2\sigma_{\varepsilon}^4 \right)
    \]
    \qed
  \end{itemize}

\end{notes}


\begin{frame}
  \frametitle{Tests d'hypothèses}
  \framesubtitle{Restriction sur les paramètres}

  \begin{theorem}\label{thm:test:single_linear_restriction}
    Soient $R$ un vecteur réel $1\times K$déterministe et $r$ un scalaire réel déterministe. Sous l'hypothèse nulle $R\beta = r$, on a~:
    \[
      \frac{R\hat{\mathrm b}-r}{\sqrt{s^2R(X'X)^{-1}R}} \sim t_{T-K}
    \]
    où $t_{T-K}$ est une loi de Student à $T-K$ degrés de liberté.
  \end{theorem}

  \bigskip

  \begin{example}
    Soit le modèle $y_t = \beta_0 + \beta_1 x_{1,t} + \beta_2 x_{2,t} + \varepsilon_t$. Pour tester l'hypothèse $\beta_1+\beta_2=1$, on pose $R = (1, 1)$ et $r=1$. Notons que pour estimer le modèle contraint nous pouvons le réécrire en substituant la contrainte~:
    \[
      y_t-x_{2,t} = \beta_0 + \beta_1\left( x_{1,t}-x_{2,t} \right) + \varepsilon_t
    \]
  \end{example}

\end{frame}



\begin{notes}

  \begin{itemize}

  \item \textbf{Preuve du théorème \ref{thm:test:single_linear_restriction}.} Le numérateur $R\hat{\mathbf b}-r$ est une variable aléatoire centrée sous l'hypothèse nulle~:
    \[
      \mathbb E \left[ R\hat{\mathbf b} - r \right] = R\mathbb E\left[ \hat{\mathbf b} \right] - r = R\beta-r = 0
    \]
    Sa variance est~:
    \[
      \mathbb V\left[ R\hat{\mathbf b}-r\right] = R\mathbb V\left[ \hat{\mathbf b} \right] R' = \sigma_{\varepsilon}^2R(X'X)^{-1}R'
    \]
    Ainsi, sous l'hypothèse nulle, on a~:
    \[
      \frac{R\hat{\mathbf b}-r}{\sigma_{\varepsilon}^2R(X'X)^{-1}R'} \sim \mathcal N(0,1)
    \]
    Mais comme nous ne connaissons pas $\sigma_{\varepsilon}^2$, on remplace cette variance par $s^2$, un estimateur sans biais. Or nous savons, par le théorème \ref{thm:s2:distribution}, que~:
    \[
      \frac{(T-K)s^2}{\sigma_{\varepsilon}^2} \sim \chi^2(T-K)
    \]
    Ainsi~:
    \[
      \frac{s^2R(X'X)^{-1}R'}{\sigma_{\varepsilon}^2R(X'X)^{-1}R'} = \frac{s^2}{\sigma_{\varepsilon}^2} \sim \frac{\chi^2(T-K)}{T-K}
    \]
    et donc~:
    \[
      \frac{R\hat{\mathbf b}-r}{s^2R(X'X)^{-1}R'} =
      \frac{\frac{R\left( \hat{\mathbf b} - \beta \right)}{\sqrt{\sigma_{\varepsilon}^2R(X'X)^{-1}R'}}}
      {\sqrt{\frac{s^2R(X'X)^{-1}R'}{\sigma_{\varepsilon}^2R(X'X)^{-1}R'}}}
    \]
    est le ratio d'une normale centrée réduite et d'un $\chi^2$ à $T-K$ degrés de liberté rapporté à $T-K$. Puisque le numérateur et le dénominateur sont des variables aléaoires indépendantes, voir la proposition \ref{prop:bhat_perp_s2}, le ratio suit une loi de student à $T-K$ degrés de liberté, voir la proposition \ref{prop:student} dans l'annexe A.\qed

  \end{itemize}

\end{notes}


\begin{frame}
  \frametitle{Tests d'hypothèses}
  \framesubtitle{Significativité d'un paramètre}

  \begin{cor}\label{cor:student_statistic}
    Soit $s_{\hat{\mathrm b}_i}^2 = s^2 \left[ (X'X)^{-1} \right]_{ii}$ la variance de l'estimateur des MCO du i-ème paramètre. La statistique~:
    \[
      t = \frac{\hat{\mathbf b}_i}{s_{\hat{\mathrm b}_i}}
    \]
    suit une loi de Student à $T-K$ degrés de liberté sous l'hypothèse nulle $\beta_i = 0$.
  \end{cor}

  \bigskip

  \begin{itemize}

  \item Conséquence directe du théorème
    \ref{thm:test:single_linear_restriction} avec $r = 0$ et $R$ un
    vecteur de sélection du i-ème élément de $\beta$ (un vecteur ligne
    nul à l'exception du i-ème élément égal à 1).\newline

  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Tests d'hypothèses}
  \framesubtitle{Restriction\underline{s} sur les paramètres}

  \begin{theorem}\label{thm:fisher_statistic}
    Soit $R$ une matrice réelle déterministe $m\times K$ de rang $m$. Soit $r$ un vecteur réel  $m\times 1$ déterministe. Alors sous l'hypothèse nulle $R\beta=r$, la statistique~:
    \[
      \frac{\nicefrac{\left(r-R\hat{\mathbf b}\right)'\left( R(X'X)^{-1}R' \right)^{-1}\left(r-R\hat{\mathbf b}\right)}{m}}{\nicefrac{SSE}{(T-K)}}
    \]
 suit une loi de Fisher $F\left(m, T-K\right)$.
  \end{theorem}

  \bigskip

  \begin{itemize}

  \item Ici on teste simultanément $m$ restrictions.\newline

  \item Ces restrictions doivent être différentes, les $m$ lignes de $R$ sont linéairement indépendantes.\newline

  \item Théorème \ref{thm:fisher_statistic} $\centernot{\Longleftrightarrow}$  $m\times$Théorème \ref{thm:test:single_linear_restriction}

  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Tests d'hypothèses}
  \framesubtitle{Restriction\underline{s} sur les paramètres}

  \begin{itemize}

  \item $r-R\hat{\mathbf b}$ est généralement différent de zéro, même
    si $R\beta = r$, à cause de l'incertitude liée à l'estimateur des
    MCO.\newline

  \item Le numérateur de la statistique de Fisher est une mesure de la distance aux $m$ contraintes.\newline

  \item Le dénominateur mesure la taille des perturbations $\varepsilon$.\newline

  \item On rejette les $m$ contraintes si la distance aux m contraintes est trop importante par rapport à la taille des perturbations.\newline

  \item Pourquoi ne pas rajouter les contraintes $R\beta=r$ à la procédure d'estimation par les MCO ou par MV~? Cela permettrait de réduire la variance de l'estimateur (moins de paramètres à estimer).

  \end{itemize}

\end{frame}


\begin{notes}

  \textbf{Preuve du théorème \ref{thm:fisher_statistic}.} Sous l'hypothèse nulle, $r=R\beta$, on a~:
    \[
      r-R\hat{\mathbf b} = R\left( \beta - \hat{\mathbf b} \right) = -R(X'X)^{-1}X'\varepsilon
    \]
    Ainsi, la variance de $r-R\hat{\mathbf b}$ est~:
    \[
      \mathbb V \left[ r-R\hat{\mathbf b} \right] = \sigma_{\varepsilon}^2R(X'X)^{-1}X'X(X'X)^{-1}R' = \sigma_{\varepsilon}^2R(X'X)^{-1}R'
    \]
    dont nous retrouvons l'inverse au centre du numérateur, au facteur
    d'échelle $\sigma_{\varepsilon}^2$ près, en notant que cette
    matrice (comme son inverse) est symétrique et donc identique à sa
    transposée. Si $r-R\hat{\mathbf b}$ mesure l'écart aux $m$
    restrictions, la statistique donne relativement plus de poids aux
    restrictions qui sont mesurées de façon plus précises. Nous pouvons donc réécrire le numérateur sous la forme $\varepsilon' Q \varepsilon$ avec~:
    \[
      Q = X(X'X)^{-1}R'\left( R(X'X)^{-1}R' \right)^{-1}R(X'X)^{-1}X'
    \]
    Clairement cette matrice est symétrique~:
    \[
      \begin{split}
        Q' &= \left( X(X'X)^{-1}R'\left( R(X'X)^{-1}R' \right)^{-1}R(X'X)^{-1}X' \right)'\\
           &= \left( R(X'X)^{-1}X' \right)'\left( R(X'X)^{-1}R' \right)^{-1}\left( X(X'X)^{-1}R' \right)'\\
           &= X(X'X)^{-1}R'\left( R(X'X)^{-1}R' \right)^{-1}R(X'X)^{-1}X'\\
           &= Q
      \end{split}
    \]
  elle est aussi idempotente~:
    {\tiny
    \[
      \begin{split}
        QQ &= X(X'X)^{-1}R'\left( R(X'X)^{-1}R' \right)^{-1}R(X'X)^{-1}X' X(X'X)^{-1}R'\left( R(X'X)^{-1}R' \right)^{-1}R(X'X)^{-1}X'\\
           &= X(X'X)^{-1}R'\left( R(X'X)^{-1}R' \right)^{-1}R(X'X)^{-1}R'\left( R(X'X)^{-1}R' \right)^{-1}R(X'X)^{-1}X'\\
           &= X(X'X)^{-1}R'\left( R(X'X)^{-1}R' \right)^{-1}R(X'X)^{-1}X' = Q
      \end{split}
    \]}
  Nous pouvons montrer que la trace de la matrice $Q$ est égale à $m$ (son rang)~:
    \[
      \begin{split}
        \mathrm{tr} (Q) &= \mathrm{tr}\left( X(X'X)^{-1}R'\left( R(X'X)^{-1}R' \right)^{-1}R(X'X)^{-1}X'  \right) \\
                         &= \mathrm{tr}\left( \left( R(X'X)^{-1}R' \right)^{-1}R(X'X)^{-1}X'X(X'X)^{-1}R'   \right)\\
                         &= \mathrm{tr}\left( \left( R(X'X)^{-1}R' \right)^{-1}R(X'X)^{-1}R'  \right)\\
                         &= \mathrm{tr} \left( I_m \right) = m
      \end{split}
    \]
    Nous savons donc que le numérateur est distribué comme $\sigma_{\varepsilon}^2\left( \nicefrac{\chi^2(m)}{m} \right)$, voir la preuve du théorème \ref{thm:s2:distribution} (le lemme utilisé dans la preuve). Par ailleurs, nous savons aussi que le dénominateur peut s'écrire comme $\nicefrac{\varepsilon' M \varepsilon}{(T-K)}$ avec $M = I-X(X'X)^{-1}X'$ une matrice symétrique, idempotente et vérifiant $\mathrm{tr}(M)=T-K$. Le dénominateur est donc distribué comme $\sigma_{\varepsilon}^2\left( \nicefrac{\chi^2\left( T-K \right)}{(T-K)} \right)$.\newline

    Comme le numérateur et le dénominateur sont deux variables aléatoires indépendantes, puisque le numérateur dépend de $\hat{\mathrm b}$ et le dénominateur de $s^2$ (voir la proposition \ref{prop:bhat_perp_s2}, $\hat{\mathrm b}$ et $s^2$ sont indépendants), on sait par définition de la loi de Fisher (voir l'annexe A) que la statistique donnée dans le théorème, dite de Fisher, suit une loi de Fisher $F(m, T-K)$.\qed

\end{notes}


\begin{frame}
  \frametitle{MCO avec contraintes linéaires}

  \begin{itemize}

  \item Le programme d'optimisation est maintenant~:
    \[
      \begin{split}
        \tilde{\mathbf b} = \arg & \min_{\mathbf b} (\mathbf{y}-X\mathbf{b})'(\mathbf{y}-X\mathbf{b})\\
                                 &\text{\underline{s.c.}} \quad R\mathbf{b} = r
      \end{split}
    \]

    \medskip

  \item Le lagrangien associé à ce programme est~:
    \[
      \mathcal L = (\mathbf{y}-X\mathbf{b})'(\mathbf{y}-X\mathbf{b}) + \bm{\lambda}'\left( R\mathbf{b} - r\right)
    \]
    où $\bm{\lambda}$ est un vecteur $m\times 1$ de multiplicateurs de Lagrange.\newline

  \item Les CNO sont~:
    \[
      \begin{cases}
        0 &= -2X'(\mathbf{y}-X\tilde{\mathbf b}) + R'\bm{\lambda}\\
        0 &= R\tilde{\mathbf b} - r
      \end{cases}
    \]

  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{MCO avec contraintes linéaires}

  \begin{itemize}

  \item On peut réécrire les CNO matriciellement sous la forme~:
    \[
      \underbrace{
      \begin{pmatrix}
        2X'X & R'\\
        R & O
      \end{pmatrix}}_{\mathbb X}
    \underbrace{
      \begin{pmatrix}
        \tilde{\mathbf b}\\
        \bm{\lambda}
      \end{pmatrix}}_{\mathbf c}
    =
    \underbrace{
      \begin{pmatrix}
        2X'\mathbf y\\
        r
      \end{pmatrix}}_{\mathbf d}
    \]

    \medskip

  \item Comme, par hypothèse, $X'X$ est une matrice $K\times K$ de plein rang et $R$ une matrice $m\times K$ de rang $m$, la matrice $\mathbb X$ est nécessairement invesible. On a donc~:
    \[
      \begin{pmatrix}
        \tilde{\mathbf b}\\
        \bm{\lambda}
      \end{pmatrix} =
      \begin{pmatrix}
        2X'X & R'\\
        R & O
      \end{pmatrix}^{-1}
      \begin{pmatrix}
        2X'\mathbf y\\
        r
      \end{pmatrix}
    \]

    \medskip

  \item Pour obtenir l'estimateur des MCO contraints $\tilde{\mathbf b}$ il faut inverser la matrice par blocs $\mathbb X$.

  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{MCO avec contraintes linéaires}

  \begin{prop}[Inversion d'une matrice partitionnée]\label{prop:inversion}
    Soient $A_{11}$ et $A_{22}$ des matrices réelles $n_1\times n_1$ et $n_2\times n_2$. On suppose que $A_{11}$ est inversible. Soient $A_{12}$ une matrice $n_1\times n_2$ et $A_{21}$ une matrice $n_2\times n_1$. Si la matrice $A$ de dimension $n\times n$, avec $n = n_1+n_2$, définie par~:
    \[
      A =
      \begin{pmatrix}
        A_{11} & A_{12}\\
        A_{21} & A_{22}
      \end{pmatrix}
    \]
    est inversible, alors on a~:
    \[
      A^{-1} =
      \begin{pmatrix}
        A_{11}^{-1}\left( I + A_{12}BA_{21}A_{11}^{-1} \right) & -A_{11}^{-1}A_{12}B \\
        -BA_{21}A_{11}^{-1} & B
      \end{pmatrix}
    \]
    avec $B = \left(A_{22}-A_{21}A_{11}^{-1}A_{12}\right)^{-1}$.
  \end{prop}

\end{frame}


\begin{notes}

  \begin{itemize}

  \item \textbf{Preuve de la proposition \ref{prop:inversion}.} Il suffit de montrer que $A^{-1}A = AA^{-1}$ est une matrice identité. On montre que $A^{1}A$ est une matrice identité (le reste est laissé au lecteur). Il suffit de montrer que les blocs $\left[ A^{-1}A \right]_{11}$ et $\left[ A^{-1}A \right]_{22}$ sont des matrices identité et que les blocs $\left[ A^{-1}A \right]_{12}$ et $\left[ A^{-1}A \right]_{21}$ sont nuls. On a~:
    \[
      \begin{split}
        \left[A^{-1}A\right]_{11} &= A_{11}^{-1}\left( I + A_{12}BA_{21}A_{11}^{-1} \right)A_{11}-A_{11}A_{12}BA_{21}\\
                                  &= I + A_{11}^{-1}A_{12}BA_{21}-A_{11}^{-1}A_{12}BA_{21}\\
                                  &= I\\
        \left[ A^{-1}A \right]_{22} &= BA_{21}A_{11}^{-1}A_{12}+BA_{22}\\
                                  &= B\left( A_{22}- A_{21}A_{11}^{-1}A_{12}\right)\\
                                  &= I\quad\text{par définition de }B\\
        \left[ A^{-1}A \right]_{12} &= A_{11}^{-1}\left( I + A_{12}BA_{21}A_{11}^{-1} \right)A_{12} - A_{11}^{-1}A_{12}BA_{22}\\
                                  &= A_{11}^{-1}\left( A_{12} + A_{12}BA_{21}A_{11}^{-1}A_{12} - A_{12} B A_{22}  \right)\\
                                  &= A_{11}^{-1}\left( A_{12} - A_{12}B \left(A_{22}- A_{21}A_{11}^{-1}A_{12} \right) \right)\\
                                  &= A_{11}^{-1}\left( A_{12} - A_{12}\right) = 0\\
        \left[A^{-1}A\right]_{21} &= -BA_{21}A_{11}^{-1}A_{11}+BA_{21}\\
                                  &= B\left( A_{21}- A_{21}\right) = 0
      \end{split}
    \]
    \qed
  \end{itemize}

\end{notes}


\begin{frame}
  \frametitle{MCO avec contraintes linéaires}

  \begin{theorem}\label{thm:constrained_ols}
    L'estimateur des MCO contraints est donné par~:
    \[
      \tilde{\mathbf b} = \hat{\mathbf b} - (X'X)^{-1}R'\left( R(X'X)^{-1}R' \right)^{-1}\left( R\hat{\mathbf b}-r \right)
    \]
    Sa variance est~:
    \[
      \mathbb V\left[ \tilde{\mathbf b} \right] = \sigma_{\varepsilon}^2(X'X)^{-1} - \sigma_{\varepsilon}^2(X'X)^{-1}R'\left( R(X'X)^{-1}R' \right)^{-1}R(X'X)^{-1}
    \]
  \end{theorem}

  \bigskip

  \begin{itemize}

  \item La variance de $\tilde{\mathbf b}$ est plus petite que celle de $\hat{\mathbf b}$.\newline

  \item L'estimateur est sans biais ssi la contrainte $R\beta = r$ est vraie.

  \end{itemize}

\end{frame}


\begin{notes}

  \begin{itemize}

  \item \textbf{Preuve du théorème \ref{thm:constrained_ols}.} En
    utilisant la formule d'inversion par bloc, voir la proposition
    \ref{prop:inversion}, on obtient~:
    {\tiny\[
      \mathbb X^{-1} =
      \begin{pmatrix}
        \frac{1}{2}(X'X)^{-1}\left[I-R'\left(R(X'X)^{-1}R'\right)^{-1}R(X'X)^{-1}\right] & (X'X)^{-1}R'\left(R(X'X)^{-1}R'\right)^{-1}\\
        \left(R(X'X)^{-1}R'\right)^{-1}R(X'X)^{-1} & -2\left(R(X'X)^{-1}R'\right)^{-1}
      \end{pmatrix}
    \]}
    Le produit scalaire du premier bloc de ligne de $\mathbb X^{-1}$ avec le vecteur $\mathbf d$ donne l'estimateur $\tilde{\mathbf b}$~:
    {\tiny
      \[
        \begin{split}
          \tilde{\mathbf b} &= \frac{1}{2}(X'X)^{-1}\left[I-R'\left(R(X'X)^{-1}R'\right)^{-1}R(X'X)^{-1}\right]2X'\mathbf y + (X'X)^{-1}R'\left(R(X'X)^{-1}R'\right)^{-1}r\\
                            &= \hat{\mathbf b} - (X'X)^{-1}R'\left(R(X'X)^{-1}R'\right)^{-1}R\hat{\mathbf b} + (X'X)^{-1}R'\left(R(X'X)^{-1}R'\right)^{-1}r\\
                            &= \hat{\mathbf b} - (X'X)^{-1}R'\left(R(X'X)^{-1}R'\right)^{-1}\left(R\hat{\mathbf b}-r\right)
        \end{split}
    \]}
  Pour calculer la variance de $\tilde{\mathbf b}$, il suffit de rappeler que si $A$ et $B$ sont deux variables aléatoires alors~:
  \[
    \mathbb V\left[ A - B \right] = \mathbb V\left[ A \right] + \mathbb V\left[ B \right] -2 \mathrm{cov}\left(A, B\right)
  \]
  On connaît déjà la variance de $\hat{\mathbf b}$, la variance du second terme est~:
  {\tiny
  \[
    \begin{split}
      \mathbb V&\left[ (X'X)^{-1}R'\left(R(X'X)^{-1}R'\right)^{-1}\left(R\hat{\mathbf b}-r\right) \right] = \mathbb V\left[ (X'X)^{-1}R'\left(R(X'X)^{-1}R'\right)^{-1}R\hat{\mathbf b} \right]\\
      &= (X'X)^{-1}R'\left(R(X'X)^{-1}R'\right)^{-1}R\sigma_{\varepsilon}^2(X'X)^{-1}R'\left(R(X'X)^{-1}R'\right)^{-1}R(X'X)^{-1}\\
      &=\sigma_{\varepsilon}^2(X'X)^{-1}R'\left(R(X'X)^{-1}R'\right)^{-1}R(X'X)^{-1}
    \end{split}
  \]}
Enfin, on a~:
{\tiny
\[
  \begin{split}
    \mathrm{cov}\left(\hat{\mathbf b}, (X'X)^{-1}R'\left(R(X'X)^{-1}R'\right)^{-1}R\hat{\mathbf b}\right) &= (X'X)^{-1}R'\left(R(X'X)^{-1}R'\right)^{-1}R\mathbb V\left[ \hat{\mathbf b} \right]\\
    &= \sigma_{\varepsilon}^2(X'X)^{-1}R'\left(R(X'X)^{-1}R'\right)^{-1}R(X'X)^{-1}
  \end{split}
\]}
L'expression de la variance de $\tilde{\mathbf b}$ s'obtient directement en sommant les variances de $\hat{\mathbf b}$ et $(X'X)^{-1}R'\left(R(X'X)^{-1}R'\right)^{-1}\left(R\hat{\mathbf b}-r\right)$, puis en retranchant deux fois la covariance.\qed
\end{itemize}

\end{notes}


\begin{frame}
  \frametitle{MV avec contraintes linéaires}

  \begin{theorem}\label{thm:constrained_ml}
    L'estimateur du maximum de vraisemblance de $\beta$ sous les contraintes $R\beta=r$ est donné par~:
    \[
      \hat{\beta}^c = \hat{\mathbf b} - (X'X)^{-1}R'\left( R(X'X)^{-1}R' \right)^{-1}\left( R\hat{\mathbf b}-r \right)
    \]
    L'estimateur du maximum de vraisemblance de $\sigma_{\varepsilon}^2$ est~:
    \[
      \tilde{\sigma}_{\varepsilon}^2 = \frac{1}{T}\left(\mathbf y - X\hat{\beta}^c\right)'\left(\mathbf y - X\hat{\beta}^c\right)
    \]
  \end{theorem}

  \bigskip

  \begin{itemize}

  \item Comme dans le cas sans contrainte, l'estimateur du MV de $\beta$ est identique à l'estimateur des MCO.\newline

  \end{itemize}

\end{frame}


\begin{notes}

  \textbf{Preuve du théorème \ref{thm:constrained_ml}.} La vraisemblance est donnée par~:
  \[
    l\left( \beta, \sigma_{\varepsilon}^2; \mathbf y \right) = -\frac{T}{2}\log 2\pi - \frac{T}{2} \log\sigma_{\varepsilon}^2 - \frac{1}{2\sigma_{\varepsilon}^2}\left( \mathbf y - X\beta \right)'\left( \mathbf y - X\beta \right)
  \]
  puisque les contraintes ne concernent pas la variance de $\varepsilon$, on obtient l'estimateur du MV de $\sigma_{\varepsilon}^2$ en annulant la dérivée partielle de la log vraissemblance par rapport à $\sigma_{\varepsilon}^2$~:
  \[
    \frac{\partial l\left( \beta, \sigma_{\varepsilon}^2; \mathbf y \right)}{\partial \sigma_{\varepsilon}^2} = -\frac{T}{2\tilde\sigma_{\varepsilon}^2} + \frac{1}{2\tilde\sigma_{\varepsilon}^4}\left(\mathbf y - X\beta \right)'\left(\mathbf y - X\beta\right) = 0
  \]
  \[
    \Leftrightarrow \tilde \sigma_{\varepsilon}^2(\beta) = \frac{1}{T}\left(\mathbf y - X\beta \right)'\left(\mathbf y - X\beta  \right)
  \]
  En substituant l'estimateur de $\sigma_{\varepsilon}^2$ dans l'expression de la log vraisemblance, on obtient la log vraisemblance concentrée~:
  \[
    l\left( \beta ; \mathbf y \right) = -\frac{T}{2}\log 2\pi - \frac{T}{2}\log\left[\left(\mathbf y - X\beta \right)'\left(\mathbf y - X\beta  \right)\right] - \frac{T}{2}
  \]
  que nous devons maximiser, par rapport $\beta$, sous la contrainte $R\beta=r$ pour obtenir l'estimateur $\hat\beta^c$. Clairement cela revient à minimiser $\left(\mathbf y - X\beta \right)'\left(\mathbf y - X\beta  \right)$ sous la contrainte $R\beta=r$. Posons le lagrangien associé à ce programme d'optimisation~:
  \[
    \mathcal L = \mathbf y'\mathbf y - 2\beta'X'\mathbf y + \beta'X'X\beta + \lambda'\left( R\beta-r\right)
  \]
  où $\lambda$ est un vecteur $m\times 1$ de multiplicateurs de Lagrange. Les conditions nécessaires d'optimalité sont~:
  \[
    \begin{cases}
      -2X'\mathbf y + 2X'X\hat\beta^c + R'\hat\lambda = 0\\
      R\hat\beta^c -r = 0

    \end{cases}
  \]
\[
    \Leftrightarrow\begin{cases}
      -2R(X'X)^{-1}X'\mathbf y + 2R\hat\beta^c + R(X'X)^{-1}R'\hat\lambda = 0\\
      R\hat\beta^c -r = 0

    \end{cases}
  \]
  \[
    \Leftrightarrow\begin{cases}
      -2R\hat{\mathbf b} + 2R\hat\beta^c + R(X'X)^{-1}R'\hat\lambda = 0\\
      R\hat\beta^c -r = 0

    \end{cases}
  \]
  De la première équation nous déduisons $\hat{\lambda}$~:
  \[
    \hat\lambda = \left( R(X'X)^{-1}R' \right)^{-1}\left( 2R\hat{\mathbf b} - 2R\hat{\beta}^c \right)
  \]
  \[
    \Leftrightarrow \hat{\lambda} = -2 \left( R(X'X)^{-1}R' \right)^{-1}\left( r - R\hat{\mathbf b}\right)
  \]
  en substituant la seconde équation. Le multiplicateur de Lagrange est non nul si et seulement si l'estimateur des MCO ne satisfait les $m$ contraintes linéaires. En substituant $\hat\lambda$ dans la première équation et en résolvant pour $\hat{\beta}^c$ on obtient~:
\[
      \hat{\beta}^c = \hat{\mathbf b} - (X'X)^{-1}R'\left( R(X'X)^{-1}R' \right)^{-1}\left( R\hat{\mathbf b}-r \right)
    \]
    \qed

\end{notes}


\begin{frame}[c]
  \frametitle{Tests du ratio de vraisemblance}

  \begin{theorem}\label{thm:likelihood_ratio_test}
    La statistique de Fisher définie dans le théorème \ref{thm:fisher_statistic} s'interprête comme un test
    de ratio de vraisemblance de $R\beta=r$ contre $R\beta\neq r$.
  \end{theorem}

\end{frame}


\begin{notes}

  \textbf{Preuve du théorème \ref{thm:likelihood_ratio_test}.} La fonction de vraisemblance est~:
  \[
    L\left(\beta, \sigma_{\varepsilon}^2;\mathbf y\right) = (2\pi)^{-\frac{T}{2}}\left(\sigma_{\varepsilon}^2\right)^{-\frac{T}{2}}\exp\left\{ -\frac{1}{\sigma_{\varepsilon}^2}\left( \mathbf y - X\beta \right)'\left( \mathbf y - X\beta \right) \right\}
  \]
  Sous l'hypothèse nulle, $R\beta=r$, la valeur du maximum de vraisemblance est~:
  \[
    (2\pi)^{-\frac{T}{2}}\left[ \frac{1}{T}\left(\mathbf y - X\hat\beta^c \right)'\left( \mathbf y - X\hat\beta^c \right) \right]^{-\frac{T}{2}}\exp\left\{-\frac{T}{2}\right\}
  \]
  Sous l'hypothèse alternative, $R\beta\neq r$, la valeur du maximum de vraisemblance est~:
\[
    (2\pi)^{-\frac{T}{2}}\left[ \frac{1}{T}\left(\mathbf y - X\hat\beta \right)'\left( \mathbf y - X\hat\beta \right) \right]^{-\frac{T}{2}}\exp\left\{-\frac{T}{2}\right\}
  \]
  La statistique du ratio de vraisemblance est donc~:
  \[
    \frac{(2\pi)^{-\frac{T}{2}}\left[ \frac{1}{T}\left(\mathbf y - X\hat\beta \right)'\left( \mathbf y - X\hat\beta \right) \right]^{-\frac{T}{2}}\exp\left\{-\frac{T}{2}\right\}}{(2\pi)^{-\frac{T}{2}}\left[ \frac{1}{T}\left(\mathbf y - X\hat\beta^c \right)'\left( \mathbf y - X\hat\beta^c \right) \right]^{-\frac{T}{2}}\exp\left\{-\frac{T}{2}\right\}}
  \]
  Le test du ratio de vraisemblance nous aménera à rejeter l'hypothèse nulle si la statistique est grande (les contraintes sont peu vraisemblables), c'est-à-dire si le ratio~:
  \[
    \frac{\left(\mathbf y - X\hat\beta^c \right)'\left( \mathbf y - X\hat\beta^c \right)}{\left(\mathbf y - X\hat\beta\right)'\left( \mathbf y - X\hat\beta\right)}
  \]
  est grand (la somme des carrés des résidus du modèle contraint est grande par rapport à la somme des carrés des résidus non contraints).\newline

  Par ailleurs, on a~:
  \[
    \begin{split}
      \left(\mathbf y - X\hat\beta^c\right)'\left(\mathbf y - X\hat\beta^c\right) &= \mathbf y'\mathbf y - 2\left.\hat\beta^c\right.'X'\mathbf y + \left.\hat\beta^c\right.'X'X \hat\beta^c\\
                                                                                  &= \mathbf y'\mathbf y - 2\hat\beta'X'\mathbf y + \left.\hat\beta\right.'X'X \hat\beta\\ &\quad + 2\left(\hat\beta-\hat\beta^c \right)'X'\mathbf y - \left.\hat\beta\right.'X'X \hat\beta + \left.\hat\beta^c\right.'X'X \hat\beta^c\\
      &= \mathbf y'\mathbf y - 2\hat\beta'X'\mathbf y + \left.\hat\beta\right.'X'X \hat\beta\\ &\quad + 2\left(\hat\beta-\hat\beta^c \right)'X'X\hat\beta - \left.\hat\beta\right.'X'X \hat\beta + \left.\hat\beta^c\right.'X'X \hat\beta^c\\
      &= \left(\mathbf y - X\hat\beta\right)'\left(\mathbf y - X\hat\beta\right) + \left(\hat\beta-\hat\beta^c\right)'X'X\left(\hat\beta-\hat\beta^c\right)
    \end{split}
  \]
  Le test du ratio de vraisemblance nous aménera donc à rejeter l'hypothèse nulle si le ratio~:
  \[
    \frac{\left(\hat\beta-\hat\beta^c\right)'X'X\left(\hat\beta-\hat\beta^c\right)}{\left(\mathbf y - X\hat\beta\right)'\left(\mathbf y - X\hat\beta\right)}
  \]
  est grand. Finalement, en utilisant la définition de l'estimateur contraint, on a~:
  \[
    \hat\beta - \hat\beta^c = (X'X)^{-1}R'\left( R(X'X)^{-1}R' \right)^{-1}\left( R\hat{\beta}-r \right)
  \]
  en substituant dans le numérateur on voit que le test du ratio de vraisemblance rejettera la nulle lorsque le ratio~:
  \[
    \frac{\left(r-R\hat{\beta}\right)'\left( R(X'X)^{-1}R' \right)^{-1}\left(r-R\hat{\beta}\right)}{SSE}
  \]
  au facteur $\nicefrac{T-K}{m}$ près, on reconnaît la statistique donnée dans le théorème \ref{thm:fisher_statistic}.\qed

\end{notes}


\begin{frame}
  \frametitle{Test joint sur les paramètres}

  \begin{cor}\label{cor:equality_test}
    Sous l'hypothèse nulle $\beta = \beta^{\star}$ la statistique~:
    \[
      \frac{\left(\hat{\mathbf b}-\beta^{\star}\right)'X'X\left(\hat{\mathbf b}-\beta^{\star}\right)}{SSE}\frac{T-K}{K}
    \]
    suit une loi de Fisher de degrés de liberté $K$ et $T-K$.
  \end{cor}

  \bigskip\bigskip

  \textbf{Preuve du corollaire \ref{cor:equality_test}} Direct par le théorème \ref{thm:fisher_statistic} avec $R$ une matrice identité, $r=\beta^{\star}$ et $m=K$.\qed

\end{frame}


\begin{frame}
  \frametitle{Test joint sur un sous ensemble des paramètres}

  \begin{prop}\label{prop:test-subset-parameters}
    Partitionnons les régresseurs et paramètres en considérant le modèle suivant~:
    \[
      \mathbf y = X_1\beta_1 + X_2\beta_2 + \epsilon
    \]
    où $X_1$ et $X_2$ sont respectivement des matrices $T\times K_1$ et $T\times K_2$, les vecteurs $\beta_1$ et $\beta_2$ sont respectivement des vecteurs $K_1\times 1$ et $K_2\times 1$. Soient $SSE_1$ la somme des carrés des résidus quand $\mathbf y$ est régressé seulment sur $X_1$ (le modèle contraint) et $SSE_{12}$ la somme des carrés des résidus quand $\mathbf y$ est régressé sur toutes les variables explicatives (le modèle non contraint). Alors, sous l'hypothèse nulle $\beta_2=0$ la statistique~:
    \[
\frac{\nicefrac{\left(SSE_1-SSE_{12}\right)}{K_2}}{\nicefrac{SSE_{12}}{T-K_1-K_2}}
    \]
    suit une loi de Fisher à $K_2$ et $T-K_1-K_2$ degrés de liberté.
  \end{prop}

\end{frame}


\begin{notes}

  \textbf{Preuve de la proposition \ref{prop:test-subset-parameters}.} Nous avons~:
  \[
    SSE_1 - SSE_{12} = \mathbf y' M_1\mathbf y - \mathbf y' M \mathbf y
  \]
  avec $M_1 = I_{T}-X_1(X_1'X_1)^{-1}X_1'$ et $M = I_{T}-X(X'X)^{-1}X'$ où  $X = \begin{pmatrix}X_1 & X_2\end{pmatrix}$. En développant, il vient~:
  \[
    \begin{split}
      SSE_1 - SEE_{12} &= \left(X_1\beta_1 + X_2\beta_2 + \varepsilon\right)'M_1\left(X_1\beta_1 + X_2\beta_2 + \varepsilon\right) - \varepsilon'M\varepsilon\\
                       &= \beta_1'X_1'M_1X_1\beta_1 + \beta_1'X_1'M_1X_2\beta_2+ \beta_1'X_1'M_1\varepsilon\\
                       &\quad + \beta_2'X_2'M_1X_1\beta_1 + \beta_2'X_2'M_1X_2\beta_2+ \beta_2'X_2'M_1\varepsilon\\
                       &\quad + \varepsilon'M_1X_1\beta_1 + \varepsilon'M_1X_2\beta_2+ \varepsilon'M_1\varepsilon - \varepsilon'M\varepsilon\\
      &= \beta_2'X_2'M_1X_2\beta_2 + \beta_2'X_2'M_1\varepsilon + \varepsilon'M_1X_2\beta_2 + \varepsilon'M_1\varepsilon - \varepsilon'M\varepsilon
    \end{split}
  \]
  en notant que $M_1X_1=0$ et $X_1'M_1 = 0$. Sous l'hypothèse nulle $\beta_2=0$ nous avons donc~:
  \[
    SSE_1 - SSE_{12} = \varepsilon'\left( M_1 - M \right)\varepsilon
  \]
  Puisque $M_1$ et $M$ sont des matrices symétriques, $M_1-M$ est nécessairement une matrice symétrique. On peut aussi montrer que cette matrice est idempotente. En effet~:
  \[
    \left( M_1-M \right)^2 = M_1^2 - M_1M - MM_1 + M^2 = M_1 - 2M_1M + M
  \]
  puisque les matrices $M_1$ et $M$ sont symériques et idempotentes. Pour que la matrice $M_1-M$ soit idempotente il faut et il suffit donc que $2M_1M$ soit égal à $-M$. On a~:
  \[
    M_1M = M - X_1(X_1'X_1)^{-1}X_1'M
  \]
  On sait que $X'M=0$, a fortiori $X_1'M = 0$ et donc $M_1M=M$ et $\left(M_1-M\right)^2=M_1-M$. Puisque $M_1-M$ a une trace éqale à $T-K_1-(T-K_1-K_2)=K_2$ il s'ensuit que $\nicefrac{\left(SSE_1-SSE_{12} \right)}{K_2}$ comme $\frac{\sigma_{\varepsilon}^2}{K_2}\chi^2(K_2)$. Par ailleurs nous savons déjà que $\nicefrac{SSE_{12}}{T-K_1-K_2}$ est distribué comme $\frac{\sigma_{\varepsilon}^2}{T-K_1-K_2}\chi^2(T-K_1-K_2)$. Enfin, comme le numérateur et le dénominateur de la statistique de test sont indépendants, car~:
  \[
    (M_1-M)M = M - M = 0
  \]
la statistique est un ratio de deux variables aléatoires du $\chi^2$ indépendantes et suit donc une loi de Fisher comme annoncée dans la proposition.\qed

\end{notes}


\begin{frame}
  \frametitle{Test de significativité jointes des pentes}

  \begin{itemize}

  \item Supposons que le modèle contienne une constante $\rightarrow$ $X_1$\newline

  \item On veut tester $\beta_2=\beta_3=\dots=\beta_K=0$ contre au moins un paramètre différent de zéro.\newline

  \item On utilise la statistique définie dans la proposition \ref{prop:test-subset-parameters}.\newline

  \item Ici on a $SSE_1 = SST$ et donc $SSE_1-SSE_{12} = SST - SSE = SSR$.\newline

  \item Ainsi la statistique s'écrit~:
    \[
      \frac{\nicefrac{SSR}{K-1}}{\nicefrac{SSE}{T-K}} = \frac{R^2}{1-R^2}\frac{T-K}{K-1}
    \]
    puisque $SSE = (1-R^2)SST$ et $SSR = R^2SST$. On accepte l'hypothèse nulle si le $R^2$ est assez proche de zéro.

  \end{itemize}


\end{frame}


\begin{notes}

  \begin{center}
    \begin{tabular}{c}
      \\
      \Huge{\textsc{Références}}\\
      \\
    \end{tabular}
  \end{center}

  \bigskip

  \nocite{Green2017}

  \nocite{Schmidt1976}

  \printbibliography

\end{notes}



\begin{frame}
  \begin{center}
    \Huge{\textsc{Annexe A}}\\
    \Huge\textbf{Probabilités}
  \end{center}
\end{frame}

\subsection{Distributions utiles}

\begin{frame}
  \frametitle{La loi normale univariée}

  \begin{defn}{}
    Si $X$ suit une normale d'espérance $\mu$ et de variance $\sigma^2$, on note $X\sim\mathcal N \left(\mu, \sigma^2\right)$, alors sa fonction de densité de probabilité est~:
    \[
      f_X(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2\sigma^2}\left(x-\mu\right)^2}
    \]
  \end{defn}

\end{frame}

\begin{frame}
  \frametitle{La loi normale multivariée}

  \begin{defn}{}
    Le vecteur aléatoire $\bm{X}\in \mathbb R^n$ suit une normale d'espérance $\bm \mu$ et de variance $\Sigma$, on note $\bm{X}\sim\mathcal N \left(\bm\mu, \Sigma\right)$, alors sa fonction de densité de probabilité est~:
    \[
      f_{\bm X}(\bm x) = \left( 2\pi \right)^{-\frac{n}{2}}\left| \Sigma \right|^{-\frac{1}{2}}e^{-\frac{1}{2}\left(\bm x-\bm\mu\right)'\Sigma^{-1}\left(\bm x-\bm\mu\right)}
    \]
  \end{defn}

\end{frame}


\begin{frame}
  \frametitle{Loi du $\chi^2$}

  \begin{defn}{}
    Soit $X_1, \dots, X_k$ une suite de variable aléaoires normales centrées-réduites indépendantes. Alors $\sum_{i=1}^kX_i^2$ suit une loi du chi-deux à $k$ degrés de liberté, notée~: $\chi^2(k)$.
  \end{defn}

  \bigskip

  \begin{prop}\label{prop:chi2}
    La fonction de densité de probabilité de $X\sim\chi^2(k)$,
    avec $k\in\mathbb N$, est~:
    \[
      f_X(x) = \frac{x^{\frac{k}{2}-1}e^{-\frac{x}{2}}}{2^{\frac{k}{2}}\Gamma\left(\frac{k}{2}\right)}
    \]
    pour tout $x>0$ et 0 sinon, où $\Gamma(z) = \int_0^{\infty}t^{z-1}e^{-t}\mathrm dt$ est la
    fonction Gamma. On a $\mathbb E\left[ X \right]=k$ et $\mathbb V\left[ X \right] = 2k$.
  \end{prop}

\end{frame}


\begin{frame}
  \frametitle{Loi du $\chi^2$}

  \bigskip

  \begin{center}
    \input{images/chapitre-1/chi-squared.tex}
  \end{center}


\end{frame}


\begin{frame}
  \frametitle{Loi de Student}

  \begin{defn}{}
    Soient $Z$ une variable aléatoire normale centrée réduite et une variable aléatoire $U\perp Z$ distribuéesuivant la loi du $\chi^2$ à $k$ degrés de liberté. Alors
    \[
      X = \frac{Z}{\sqrt{\nicefrac{U}{k}}}
    \]
    suit une loi de Student à $k$ degrés de liberté, notée $t_k$.
  \end{defn}

  \bigskip

  \begin{prop}\label{prop:student}
    La fonction de densité de probabilité de $X\sim t_k$,
    avec $k\in\mathbb N$, est~:
    \[
      f_X(x) =
      \frac{1}{\sqrt{k\pi}}\frac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k}{2})}\left(1+\frac{x^2}{k}\right)^{-\frac{k+1}{2}}
    \]
    pour tout $x\in\mathbb R$, où $\Gamma(z) = \int_0^{\infty}t^{z-1}e^{-t}\mathrm dt$ est la
    fonction Gamma. On a $\mathbb E\left[ X \right]=0$ si $k>1$ et $\mathbb V\left[ X \right] = \frac{k}{k-2}$ si $k>2$. La loi de student converge vers la loi normale si $k$ tend vers l'infini.
  \end{prop}

\end{frame}


\begin{frame}
  \frametitle{Loi de Student}

  \bigskip

  \begin{center}
    \input{images/chapitre-1/student.tex}
  \end{center}


\end{frame}



\begin{frame}
  \frametitle{Loi de Fisher}

  \begin{defn}{}
    Soient $U\sim\chi^2(p)$ et $V\sim\chi^2(q)$, avec  $U\perp V$. $X = \frac{\nicefrac{U}{p}}{\nicefrac{V}{q}}$ une variable aléatoire réelle distribuée selon la loi de Fisher de degrés de liberté $p$ et $q$. On note la distribution de Fisher~: $F(p,q)$.
  \end{defn}

  \bigskip

  \begin{prop}\label{prop:fisher}
    La fonction de densité de probabilité de $X\sim F(p,q)$,
    avec $(p,q)\in\mathbb N^{\star}\times \mathbb N^{\star}$, est~:
    \[
      f_X(x) = \frac{\left( \frac{px}{px+q} \right)^{\frac{p}{2}}\left( 1-\frac{px}{px+q} \right)^{\frac{q}{2}}}{x B\left(\frac{p}{2},\frac{q}{2}\right)}
    \]
    pour tout $x\in\mathbb R_+$, où $B(\alpha,\beta) = \int_0^1t^{\alpha-1}(1-t)^{\beta-1}\mathrm dt$ est la
    fonction Beta. On a $\mathbb E\left[ X \right]=\frac{q}{q-2}$ si $q>2$ et $\mathbb V\left[ X \right] = \frac{2q^2\left( p+q-2 \right)}{p\left( q-2 \right)^2\left( q-4 \right)}$ si $q>4$.
  \end{prop}

\end{frame}


\begin{frame}
  \frametitle{Loi de Fisher}

  \bigskip

  \begin{center}
    \input{images/chapitre-1/fisher-snedecor-1.tex}
  \end{center}


\end{frame}


\begin{frame}
  \frametitle{Loi de Fisher}

  \bigskip

  \begin{center}
    \input{images/chapitre-1/fisher-snedecor-2.tex}
  \end{center}


\end{frame}


\begin{frame}
  \frametitle{Loi de Fisher}

  \bigskip

  \begin{center}
    \input{images/chapitre-1/fisher-snedecor-3.tex}
  \end{center}


\end{frame}



\end{document}


% Local Variables:
% ispell-check-comments: exclusive
% ispell-local-dictionary: "french"
% TeX-master: t
% End:
